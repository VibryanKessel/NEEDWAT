* 
* ==> Audit <==
* |--------------|--------------------------|----------|----------------------|---------|---------------------|---------------------|
|   Command    |           Args           | Profile  |         User         | Version |     Start Time      |      End Time       |
|--------------|--------------------------|----------|----------------------|---------|---------------------|---------------------|
| start        |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 19:45 CET | 14 Nov 23 19:47 CET |
| service      | myservice --url          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 21:26 CET | 14 Nov 23 21:26 CET |
| dashboard    |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 21:27 CET |                     |
| service      | myservice2 --url         | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 21:41 CET | 14 Nov 23 21:42 CET |
| dashboard    | --url                    | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 21:42 CET |                     |
| addons       | enable ingress           | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 21:56 CET | 14 Nov 23 21:57 CET |
| addons       | enable ingress           | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 21:59 CET | 14 Nov 23 21:59 CET |
| addons       | enable ingress-dns       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 22:17 CET | 14 Nov 23 22:17 CET |
| tunnel       |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 22:17 CET | 14 Nov 23 22:19 CET |
| tunnel       |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 22:19 CET | 14 Nov 23 22:20 CET |
| tunnel       |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 14 Nov 23 22:22 CET | 14 Nov 23 22:27 CET |
| start        | --driver=docker          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 11:58 CET |                     |
| start        | --driver=docker          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 11:58 CET |                     |
| start        |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 11:58 CET |                     |
| start        | --driver=docker          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 12:01 CET |                     |
| start        | --driver=docker          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 12:03 CET | 17 Jan 24 12:04 CET |
| service      | frontend-service -n prod | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 13:01 CET |                     |
| service      | frontend-service -n prod | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 13:03 CET |                     |
| service      | frontend-service -n prod | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 13:04 CET |                     |
| service      | list                     | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 13:04 CET | 17 Jan 24 13:04 CET |
| service      | list                     | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 13:06 CET | 17 Jan 24 13:06 CET |
| service      | frontend-service         | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 17 Jan 24 13:07 CET |                     |
| start        |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 24 Jan 24 21:07 CET | 24 Jan 24 21:09 CET |
| update-check |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 24 Jan 24 21:14 CET | 24 Jan 24 21:14 CET |
| service      | list                     | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 24 Jan 24 21:50 CET | 24 Jan 24 21:50 CET |
| service      |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 24 Jan 24 21:50 CET |                     |
| service      | api-service --url        | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 24 Jan 24 21:50 CET | 26 Jan 24 17:05 CET |
| update-check |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 24 Jan 24 22:19 CET | 24 Jan 24 22:19 CET |
| update-check |                          | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 26 Jan 24 15:14 CET | 26 Jan 24 15:14 CET |
| service      | api-service --url        | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 26 Jan 24 17:17 CET |                     |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 26 Jan 24 17:26 CET | 26 Jan 24 17:28 CET |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 26 Jan 24 17:28 CET | 28 Jan 24 03:38 CET |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 28 Jan 24 03:54 CET | 28 Jan 24 03:55 CET |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 28 Jan 24 03:59 CET |                     |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 28 Jan 24 03:59 CET |                     |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 28 Jan 24 04:01 CET |                     |
| service      | site-service --url       | minikube | DESKTOP-ABORQK5\ZERO | v1.32.0 | 28 Jan 24 04:10 CET |                     |
|--------------|--------------------------|----------|----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/24 21:07:32
Running on machine: DESKTOP-ABORQK5
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0124 21:07:32.715203   25360 out.go:296] Setting OutFile to fd 84 ...
I0124 21:07:32.716939   25360 out.go:348] isatty.IsTerminal(84) = true
I0124 21:07:32.716939   25360 out.go:309] Setting ErrFile to fd 88...
I0124 21:07:32.716939   25360 out.go:348] isatty.IsTerminal(88) = true
W0124 21:07:32.757993   25360 root.go:314] Error reading config file at C:\Users\ZERO\.minikube\config\config.json: open C:\Users\ZERO\.minikube\config\config.json: The system cannot find the file specified.
I0124 21:07:32.785340   25360 out.go:303] Setting JSON to false
I0124 21:07:32.800832   25360 start.go:128] hostinfo: {"hostname":"DESKTOP-ABORQK5","uptime":198130,"bootTime":1705928722,"procs":394,"os":"windows","platform":"Microsoft Windows 10 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.3930 Build 19045.3930","kernelVersion":"10.0.19045.3930 Build 19045.3930","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"f6bfa9d1-77cc-4ee7-8b45-2d7c09f13874"}
W0124 21:07:32.801347   25360 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0124 21:07:32.804667   25360 out.go:177] üòÑ  minikube v1.32.0 on Microsoft Windows 10 Home 10.0.19045.3930 Build 19045.3930
I0124 21:07:32.807839   25360 notify.go:220] Checking for updates...
I0124 21:07:32.825092   25360 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0124 21:07:32.832818   25360 driver.go:378] Setting default libvirt URI to qemu:///system
I0124 21:07:33.172212   25360 docker.go:122] docker version: linux-24.0.2:Docker Desktop 4.20.1 (110738)
I0124 21:07:33.192207   25360 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0124 21:07:33.960820   25360 info.go:266] docker info: {ID:2f622035-d83a-447b-86d2-2f2287512c92 Containers:6 ContainersRunning:1 ContainersPaused:0 ContainersStopped:5 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:49 OomKillDisable:true NGoroutines:22833 SystemTime:2024-01-24 20:07:33.901900387 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8022810624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.18.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.12.0]] Warnings:<nil>}}
I0124 21:07:33.962822   25360 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0124 21:07:33.963829   25360 start.go:298] selected driver: docker
I0124 21:07:33.963829   25360 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true ingress-dns:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ZERO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0124 21:07:33.964829   25360 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0124 21:07:34.013823   25360 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0124 21:07:34.491175   25360 info.go:266] docker info: {ID:2f622035-d83a-447b-86d2-2f2287512c92 Containers:6 ContainersRunning:1 ContainersPaused:0 ContainersStopped:5 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:49 OomKillDisable:true NGoroutines:22729 SystemTime:2024-01-24 20:07:34.447171192 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8022810624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.18.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.12.0]] Warnings:<nil>}}
I0124 21:07:34.650178   25360 cni.go:84] Creating CNI manager for ""
I0124 21:07:34.650178   25360 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0124 21:07:34.650178   25360 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true ingress-dns:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ZERO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0124 21:07:34.653175   25360 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0124 21:07:34.654177   25360 cache.go:121] Beginning downloading kic base image for docker with docker
I0124 21:07:34.655180   25360 out.go:177] üöú  Pulling base image ...
I0124 21:07:34.657177   25360 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0124 21:07:34.657177   25360 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0124 21:07:34.659184   25360 preload.go:148] Found local preload: C:\Users\ZERO\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0124 21:07:34.659184   25360 cache.go:56] Caching tarball of preloaded images
I0124 21:07:34.660183   25360 preload.go:174] Found C:\Users\ZERO\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0124 21:07:34.660183   25360 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0124 21:07:34.661178   25360 profile.go:148] Saving config to C:\Users\ZERO\.minikube\profiles\minikube\config.json ...
I0124 21:07:34.933389   25360 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0124 21:07:34.933389   25360 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0124 21:07:34.933389   25360 cache.go:194] Successfully downloaded all kic artifacts
I0124 21:07:34.935393   25360 start.go:365] acquiring machines lock for minikube: {Name:mkf6df63620e9585d8cdd12e69803c7516e1f781 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0124 21:07:34.935393   25360 start.go:369] acquired machines lock for "minikube" in 0s
I0124 21:07:34.935393   25360 start.go:96] Skipping create...Using existing machine configuration
I0124 21:07:34.935393   25360 fix.go:54] fixHost starting: 
I0124 21:07:34.973393   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:07:35.229156   25360 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0124 21:07:35.229156   25360 fix.go:128] unexpected machine state, will restart: <nil>
I0124 21:07:35.230163   25360 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0124 21:07:35.254164   25360 cli_runner.go:164] Run: docker start minikube
I0124 21:07:36.418498   25360 cli_runner.go:217] Completed: docker start minikube: (1.1643336s)
I0124 21:07:36.443499   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:07:36.706532   25360 kic.go:430] container "minikube" state is running.
I0124 21:07:36.737389   25360 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0124 21:07:37.013869   25360 profile.go:148] Saving config to C:\Users\ZERO\.minikube\profiles\minikube\config.json ...
I0124 21:07:37.016867   25360 machine.go:88] provisioning docker machine ...
I0124 21:07:37.016867   25360 ubuntu.go:169] provisioning hostname "minikube"
I0124 21:07:37.048665   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:37.378318   25360 main.go:141] libmachine: Using SSH client type: native
I0124 21:07:37.395318   25360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4747e0] 0x477320 <nil>  [] 0s} 127.0.0.1 57867 <nil> <nil>}
I0124 21:07:37.395318   25360 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0124 21:07:37.474317   25360 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0124 21:07:40.856197   25360 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0124 21:07:40.880825   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:41.141427   25360 main.go:141] libmachine: Using SSH client type: native
I0124 21:07:41.142423   25360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4747e0] 0x477320 <nil>  [] 0s} 127.0.0.1 57867 <nil> <nil>}
I0124 21:07:41.142423   25360 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0124 21:07:41.320603   25360 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0124 21:07:41.320603   25360 ubuntu.go:175] set auth options {CertDir:C:\Users\ZERO\.minikube CaCertPath:C:\Users\ZERO\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\ZERO\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\ZERO\.minikube\machines\server.pem ServerKeyPath:C:\Users\ZERO\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\ZERO\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\ZERO\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\ZERO\.minikube}
I0124 21:07:41.320603   25360 ubuntu.go:177] setting up certificates
I0124 21:07:41.320603   25360 provision.go:83] configureAuth start
I0124 21:07:41.341611   25360 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0124 21:07:41.582414   25360 provision.go:138] copyHostCerts
I0124 21:07:41.598413   25360 exec_runner.go:144] found C:\Users\ZERO\.minikube/ca.pem, removing ...
I0124 21:07:41.598413   25360 exec_runner.go:203] rm: C:\Users\ZERO\.minikube\ca.pem
I0124 21:07:41.599413   25360 exec_runner.go:151] cp: C:\Users\ZERO\.minikube\certs\ca.pem --> C:\Users\ZERO\.minikube/ca.pem (1070 bytes)
I0124 21:07:41.617419   25360 exec_runner.go:144] found C:\Users\ZERO\.minikube/cert.pem, removing ...
I0124 21:07:41.617419   25360 exec_runner.go:203] rm: C:\Users\ZERO\.minikube\cert.pem
I0124 21:07:41.618416   25360 exec_runner.go:151] cp: C:\Users\ZERO\.minikube\certs\cert.pem --> C:\Users\ZERO\.minikube/cert.pem (1115 bytes)
I0124 21:07:41.636421   25360 exec_runner.go:144] found C:\Users\ZERO\.minikube/key.pem, removing ...
I0124 21:07:41.636421   25360 exec_runner.go:203] rm: C:\Users\ZERO\.minikube\key.pem
I0124 21:07:41.636421   25360 exec_runner.go:151] cp: C:\Users\ZERO\.minikube\certs\key.pem --> C:\Users\ZERO\.minikube/key.pem (1679 bytes)
I0124 21:07:41.639420   25360 provision.go:112] generating server cert: C:\Users\ZERO\.minikube\machines\server.pem ca-key=C:\Users\ZERO\.minikube\certs\ca.pem private-key=C:\Users\ZERO\.minikube\certs\ca-key.pem org=ZERO.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0124 21:07:42.086565   25360 provision.go:172] copyRemoteCerts
I0124 21:07:42.124566   25360 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0124 21:07:42.144561   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:42.361557   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:07:42.478564   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0124 21:07:42.524271   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0124 21:07:42.564525   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0124 21:07:42.609398   25360 provision.go:86] duration metric: configureAuth took 1.284762s
I0124 21:07:42.609398   25360 ubuntu.go:193] setting minikube options for container-runtime
I0124 21:07:42.610616   25360 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0124 21:07:42.628188   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:42.868201   25360 main.go:141] libmachine: Using SSH client type: native
I0124 21:07:42.869477   25360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4747e0] 0x477320 <nil>  [] 0s} 127.0.0.1 57867 <nil> <nil>}
I0124 21:07:42.869477   25360 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0124 21:07:43.035269   25360 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0124 21:07:43.035269   25360 ubuntu.go:71] root file system type: overlay
I0124 21:07:43.035823   25360 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0124 21:07:43.055142   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:43.290219   25360 main.go:141] libmachine: Using SSH client type: native
I0124 21:07:43.291217   25360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4747e0] 0x477320 <nil>  [] 0s} 127.0.0.1 57867 <nil> <nil>}
I0124 21:07:43.291217   25360 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0124 21:07:43.476818   25360 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0124 21:07:43.496829   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:43.746546   25360 main.go:141] libmachine: Using SSH client type: native
I0124 21:07:43.747544   25360 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4747e0] 0x477320 <nil>  [] 0s} 127.0.0.1 57867 <nil> <nil>}
I0124 21:07:43.747544   25360 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0124 21:07:43.922549   25360 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0124 21:07:43.922549   25360 machine.go:91] provisioned docker machine in 6.9056812s
I0124 21:07:43.923826   25360 start.go:300] post-start starting for "minikube" (driver="docker")
I0124 21:07:43.923826   25360 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0124 21:07:43.958668   25360 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0124 21:07:43.978213   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:44.216546   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:07:44.384553   25360 ssh_runner.go:195] Run: cat /etc/os-release
I0124 21:07:44.394077   25360 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0124 21:07:44.394131   25360 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0124 21:07:44.394131   25360 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0124 21:07:44.394131   25360 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0124 21:07:44.394829   25360 filesync.go:126] Scanning C:\Users\ZERO\.minikube\addons for local assets ...
I0124 21:07:44.395378   25360 filesync.go:126] Scanning C:\Users\ZERO\.minikube\files for local assets ...
I0124 21:07:44.396122   25360 start.go:303] post-start completed in 472.2764ms
I0124 21:07:44.431536   25360 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0124 21:07:44.452851   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:44.684221   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:07:44.838221   25360 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0124 21:07:44.850219   25360 fix.go:56] fixHost completed within 9.9148257s
I0124 21:07:44.850219   25360 start.go:83] releasing machines lock for "minikube", held for 9.9148257s
I0124 21:07:44.876877   25360 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0124 21:07:45.118888   25360 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0124 21:07:45.137886   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:45.144891   25360 ssh_runner.go:195] Run: cat /version.json
I0124 21:07:45.166892   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:07:45.394889   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:07:45.426893   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:07:45.879772   25360 ssh_runner.go:195] Run: systemctl --version
I0124 21:07:45.936908   25360 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0124 21:07:45.981348   25360 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0124 21:07:45.999549   25360 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0124 21:07:46.033550   25360 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0124 21:07:46.054551   25360 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0124 21:07:46.054551   25360 start.go:472] detecting cgroup driver to use...
I0124 21:07:46.054551   25360 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0124 21:07:46.055559   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0124 21:07:46.117547   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0124 21:07:46.173462   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0124 21:07:46.192468   25360 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0124 21:07:46.226462   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0124 21:07:46.275461   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0124 21:07:46.331465   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0124 21:07:46.384461   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0124 21:07:46.437463   25360 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0124 21:07:46.490470   25360 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0124 21:07:46.541465   25360 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0124 21:07:46.591461   25360 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0124 21:07:46.639467   25360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0124 21:07:46.853881   25360 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0124 21:07:47.053982   25360 start.go:472] detecting cgroup driver to use...
I0124 21:07:47.053982   25360 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0124 21:07:47.095853   25360 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0124 21:07:47.124877   25360 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0124 21:07:47.166950   25360 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0124 21:07:47.192954   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0124 21:07:47.272976   25360 ssh_runner.go:195] Run: which cri-dockerd
I0124 21:07:47.331954   25360 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0124 21:07:47.391786   25360 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0124 21:07:47.531672   25360 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0124 21:07:47.861281   25360 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0124 21:07:48.050396   25360 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0124 21:07:48.050948   25360 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0124 21:07:48.119018   25360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0124 21:07:48.302855   25360 ssh_runner.go:195] Run: sudo systemctl restart docker
I0124 21:07:49.109511   25360 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0124 21:07:49.294855   25360 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0124 21:07:49.483729   25360 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0124 21:07:49.744620   25360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0124 21:07:49.960720   25360 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0124 21:07:50.029303   25360 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0124 21:07:50.228838   25360 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0124 21:07:50.638479   25360 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0124 21:07:50.675473   25360 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0124 21:07:50.686475   25360 start.go:540] Will wait 60s for crictl version
I0124 21:07:50.719472   25360 ssh_runner.go:195] Run: which crictl
I0124 21:07:50.759474   25360 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0124 21:07:50.989485   25360 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0124 21:07:51.008474   25360 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0124 21:07:51.292476   25360 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0124 21:07:51.338474   25360 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0124 21:07:51.368098   25360 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0124 21:07:51.846077   25360 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0124 21:07:51.879078   25360 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0124 21:07:51.889074   25360 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0124 21:07:51.932073   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0124 21:07:52.207557   25360 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0124 21:07:52.226548   25360 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0124 21:07:52.292554   25360 docker.go:671] Got preloaded images: -- stdout --
efrei2023/frontend:1
efrei2023/backend:1
bitnami/mysql:8.0.35-debian-11-r2
nginx:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
efrei/myservice:1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0124 21:07:52.293551   25360 docker.go:601] Images already preloaded, skipping extraction
I0124 21:07:52.335580   25360 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0124 21:07:52.396947   25360 docker.go:671] Got preloaded images: -- stdout --
efrei2023/frontend:1
efrei2023/backend:1
bitnami/mysql:8.0.35-debian-11-r2
nginx:latest
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
efrei/myservice:1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0124 21:07:52.397497   25360 cache_images.go:84] Images are preloaded, skipping loading
I0124 21:07:52.421310   25360 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0124 21:07:52.769595   25360 cni.go:84] Creating CNI manager for ""
I0124 21:07:52.774596   25360 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0124 21:07:52.774596   25360 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0124 21:07:52.774596   25360 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0124 21:07:52.775596   25360 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0124 21:07:52.775596   25360 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0124 21:07:52.840593   25360 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0124 21:07:52.877596   25360 binaries.go:44] Found k8s binaries, skipping transfer
I0124 21:07:52.939600   25360 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0124 21:07:52.976597   25360 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0124 21:07:53.015596   25360 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0124 21:07:53.059693   25360 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0124 21:07:53.150958   25360 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0124 21:07:53.160955   25360 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0124 21:07:53.189954   25360 certs.go:56] Setting up C:\Users\ZERO\.minikube\profiles\minikube for IP: 192.168.49.2
I0124 21:07:53.189954   25360 certs.go:190] acquiring lock for shared ca certs: {Name:mk791da9dfec0b9f8c8c8620d1bebc6bf99a015f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0124 21:07:53.209962   25360 certs.go:199] skipping minikubeCA CA generation: C:\Users\ZERO\.minikube\ca.key
I0124 21:07:53.242958   25360 certs.go:199] skipping proxyClientCA CA generation: C:\Users\ZERO\.minikube\proxy-client-ca.key
I0124 21:07:53.244958   25360 certs.go:315] skipping minikube-user signed cert generation: C:\Users\ZERO\.minikube\profiles\minikube\client.key
I0124 21:07:53.279957   25360 certs.go:315] skipping minikube signed cert generation: C:\Users\ZERO\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0124 21:07:53.308956   25360 certs.go:315] skipping aggregator signed cert generation: C:\Users\ZERO\.minikube\profiles\minikube\proxy-client.key
I0124 21:07:53.314961   25360 certs.go:437] found cert: C:\Users\ZERO\.minikube\certs\C:\Users\ZERO\.minikube\certs\ca-key.pem (1675 bytes)
I0124 21:07:53.314961   25360 certs.go:437] found cert: C:\Users\ZERO\.minikube\certs\C:\Users\ZERO\.minikube\certs\ca.pem (1070 bytes)
I0124 21:07:53.315959   25360 certs.go:437] found cert: C:\Users\ZERO\.minikube\certs\C:\Users\ZERO\.minikube\certs\cert.pem (1115 bytes)
I0124 21:07:53.316976   25360 certs.go:437] found cert: C:\Users\ZERO\.minikube\certs\C:\Users\ZERO\.minikube\certs\key.pem (1679 bytes)
I0124 21:07:53.324959   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0124 21:07:53.389968   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0124 21:07:53.439963   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0124 21:07:53.488965   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0124 21:07:53.534973   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0124 21:07:53.584967   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0124 21:07:53.634966   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0124 21:07:53.686964   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0124 21:07:53.728815   25360 ssh_runner.go:362] scp C:\Users\ZERO\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0124 21:07:53.782306   25360 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0124 21:07:53.844969   25360 ssh_runner.go:195] Run: openssl version
I0124 21:07:53.908966   25360 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0124 21:07:53.961968   25360 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0124 21:07:53.971963   25360 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 14 18:47 /usr/share/ca-certificates/minikubeCA.pem
I0124 21:07:54.013974   25360 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0124 21:07:54.060506   25360 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0124 21:07:54.121506   25360 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0124 21:07:54.166512   25360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0124 21:07:54.217508   25360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0124 21:07:54.265507   25360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0124 21:07:54.318506   25360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0124 21:07:54.368512   25360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0124 21:07:54.436497   25360 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0124 21:07:54.454593   25360 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true ingress-dns:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\ZERO:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0124 21:07:54.478688   25360 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0124 21:07:54.568645   25360 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0124 21:07:54.588639   25360 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0124 21:07:54.588639   25360 kubeadm.go:636] restartCluster start
I0124 21:07:54.632505   25360 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0124 21:07:54.656038   25360 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0124 21:07:54.678034   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0124 21:07:54.928044   25360 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:52000"
I0124 21:07:54.928044   25360 kubeconfig.go:135] verify returned: got: 127.0.0.1:52000, want: 127.0.0.1:57871
I0124 21:07:54.932713   25360 lock.go:35] WriteFile acquiring C:\Users\ZERO\.kube\config: {Name:mk6a5748766e88e3a509e3a4f1708fba5743243f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0124 21:07:55.014511   25360 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0124 21:07:55.033513   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:55.076512   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:55.098514   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:55.098514   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:55.132514   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:55.171473   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:55.684537   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:55.733170   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:55.759100   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:56.184099   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:56.219101   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:56.239102   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:56.685102   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:56.741839   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:56.768834   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:57.183742   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:57.258852   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:57.278853   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:57.679207   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:57.716897   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:57.734890   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:58.178144   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:58.210140   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:58.234142   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:58.677612   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:58.760944   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:58.794332   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:59.178485   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:59.231483   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:59.252489   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:07:59.680380   25360 api_server.go:166] Checking apiserver status ...
I0124 21:07:59.719385   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:07:59.745387   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:00.183383   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:00.231384   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:00.253387   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:00.686764   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:00.723764   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:00.746767   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:01.176081   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:01.215084   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:01.240026   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:01.675549   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:01.716545   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:01.737547   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:02.175248   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:02.213087   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:02.238084   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:02.678214   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:02.716221   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:02.736213   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:03.173111   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:03.211111   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:03.235106   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:03.675708   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:03.712460   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:03.746156   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:04.186896   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:04.240733   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:04.262730   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:04.682742   25360 api_server.go:166] Checking apiserver status ...
I0124 21:08:04.716749   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0124 21:08:04.735741   25360 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0124 21:08:05.041239   25360 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0124 21:08:05.041859   25360 kubeadm.go:1128] stopping kube-system containers ...
I0124 21:08:05.063643   25360 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0124 21:08:05.118651   25360 docker.go:469] Stopping containers: [532a7c4656fc 23555900f419 357be60d8238 392beaeff1bc 044a778148bc d02b56ae72fa b97e6926bf8b b0601e5a9285 2a9aacf194e8 618a2c0ae050 41c74e564dff 60e07ea32bc4 7c140e392dac e300604e0bf6 f8e0fb158faf 397560b4b935 00c4e68c561b 420bed3c0e2d c1a3f06ab04c 2ab10c3c265c 5a24c8e8f586 5b71c70fac68 a13e0b27f745 aa5f2ea79676 9059aae7a084 1194bce86eb6 aac84f4266df 1979215b9397 8450f478067d 0cbc138d27e4 49377d002b35]
I0124 21:08:05.139647   25360 ssh_runner.go:195] Run: docker stop 532a7c4656fc 23555900f419 357be60d8238 392beaeff1bc 044a778148bc d02b56ae72fa b97e6926bf8b b0601e5a9285 2a9aacf194e8 618a2c0ae050 41c74e564dff 60e07ea32bc4 7c140e392dac e300604e0bf6 f8e0fb158faf 397560b4b935 00c4e68c561b 420bed3c0e2d c1a3f06ab04c 2ab10c3c265c 5a24c8e8f586 5b71c70fac68 a13e0b27f745 aa5f2ea79676 9059aae7a084 1194bce86eb6 aac84f4266df 1979215b9397 8450f478067d 0cbc138d27e4 49377d002b35
I0124 21:08:05.230891   25360 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0124 21:08:05.313887   25360 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0124 21:08:05.341890   25360 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Nov 14 18:47 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Jan 17 11:04 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Nov 14 18:47 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan 17 11:04 /etc/kubernetes/scheduler.conf

I0124 21:08:05.383891   25360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0124 21:08:05.442890   25360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0124 21:08:05.513900   25360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0124 21:08:05.537888   25360 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0124 21:08:05.576889   25360 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0124 21:08:05.637892   25360 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0124 21:08:05.658889   25360 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0124 21:08:05.702893   25360 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0124 21:08:05.756888   25360 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0124 21:08:05.780888   25360 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0124 21:08:05.780888   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0124 21:08:06.383920   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0124 21:08:07.361287   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0124 21:08:07.715135   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0124 21:08:07.854095   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0124 21:08:08.072627   25360 api_server.go:52] waiting for apiserver process to appear ...
I0124 21:08:08.136855   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:08.195867   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:08.805663   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:09.273493   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:09.783355   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:10.294235   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:10.802856   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:11.303948   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:11.805936   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:12.310162   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:12.791505   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:13.303090   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:13.767934   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:14.281610   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:14.765522   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:15.309893   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:08:15.397310   25360 api_server.go:72] duration metric: took 7.3246835s to wait for apiserver process to appear ...
I0124 21:08:15.397310   25360 api_server.go:88] waiting for apiserver healthz status ...
I0124 21:08:15.399313   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:15.407305   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:15.407305   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:15.413310   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:15.924210   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:15.928220   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:16.414883   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:16.419878   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:16.914099   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:16.917104   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:17.429751   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:17.438184   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:17.925388   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:17.941384   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:18.420800   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:18.425066   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:18.917238   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:18.925232   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:19.420506   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:19.427180   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:19.919732   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:19.923740   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:20.419255   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:20.424256   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:20.919532   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:20.925346   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:21.414898   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:21.423946   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": EOF
I0124 21:08:21.915502   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:26.930232   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0124 21:08:26.930344   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:31.940771   25360 api_server.go:269] stopped: https://127.0.0.1:57871/healthz: Get "https://127.0.0.1:57871/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0124 21:08:31.940771   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:32.592073   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0124 21:08:32.592073   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0124 21:08:32.592073   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:33.077547   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0124 21:08:33.077547   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0124 21:08:33.077547   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:33.279309   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:33.279309   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:33.413677   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:33.585180   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:33.585894   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:33.925420   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:33.996787   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:33.996787   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:34.418884   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:34.679865   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:34.679865   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:34.915891   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:35.189836   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:35.189836   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:35.427219   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:35.579676   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:35.579676   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:35.923048   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:36.080962   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:36.080962   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:36.421894   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:36.485405   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:36.485405   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:36.922783   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:36.983782   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:36.983782   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:37.419182   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:37.487502   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:37.487575   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:37.915500   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:37.989337   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:37.989337   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:38.416646   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:38.491496   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:38.491496   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:38.914728   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:38.992862   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:38.992862   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:39.426898   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:39.576707   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:39.576725   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:39.926061   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:40.005311   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:40.005311   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:40.425179   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:40.589634   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:40.589634   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:40.921519   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:41.079099   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:41.079099   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:41.421523   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:41.484993   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:41.484993   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:41.913728   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:41.992161   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:41.992161   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:42.428171   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:42.491137   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0124 21:08:42.491137   25360 api_server.go:103] status: https://127.0.0.1:57871/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0124 21:08:42.925740   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:08:43.386376   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 200:
ok
I0124 21:08:43.587486   25360 api_server.go:141] control plane version: v1.28.3
I0124 21:08:43.587486   25360 api_server.go:131] duration metric: took 28.1901758s to wait for apiserver health ...
I0124 21:08:43.587486   25360 cni.go:84] Creating CNI manager for ""
I0124 21:08:43.587486   25360 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0124 21:08:43.590485   25360 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0124 21:08:43.651565   25360 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0124 21:08:44.883845   25360 ssh_runner.go:235] Completed: sudo mkdir -p /etc/cni/net.d: (1.2322803s)
I0124 21:08:44.883845   25360 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0124 21:08:45.785970   25360 system_pods.go:43] waiting for kube-system pods to appear ...
I0124 21:08:45.901457   25360 system_pods.go:59] 8 kube-system pods found
I0124 21:08:45.901457   25360 system_pods.go:61] "coredns-5dd5756b68-7hzfs" [62967fb2-ef09-4d0c-8ca8-a985ac49f72e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0124 21:08:45.901457   25360 system_pods.go:61] "etcd-minikube" [92c60c16-f9d6-47a2-8a0c-3c01d5180229] Running
I0124 21:08:45.901457   25360 system_pods.go:61] "kube-apiserver-minikube" [5a285dae-5a3d-4a1d-a97a-f2ad9bfcb64b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0124 21:08:45.901457   25360 system_pods.go:61] "kube-controller-manager-minikube" [4a0c9971-e680-4c8b-a85a-e61af2abe6aa] Running
I0124 21:08:45.901457   25360 system_pods.go:61] "kube-ingress-dns-minikube" [dc3765bd-98a3-4de5-80f2-a5e2b9071cab] Running / Ready:ContainersNotReady (containers with unready status: [minikube-ingress-dns]) / ContainersReady:ContainersNotReady (containers with unready status: [minikube-ingress-dns])
I0124 21:08:45.901457   25360 system_pods.go:61] "kube-proxy-bkhn7" [aaf9c56e-0e80-4fe5-8056-b0df9b7c8e87] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0124 21:08:45.901457   25360 system_pods.go:61] "kube-scheduler-minikube" [be7b784c-3645-4132-b2f3-37b5f590a81f] Running
I0124 21:08:45.901457   25360 system_pods.go:61] "storage-provisioner" [3989d12b-fc52-4bd9-b6db-a97707ba6197] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0124 21:08:45.901457   25360 system_pods.go:74] duration metric: took 115.4863ms to wait for pod list to return data ...
I0124 21:08:45.901457   25360 node_conditions.go:102] verifying NodePressure condition ...
I0124 21:08:45.995114   25360 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0124 21:08:45.995114   25360 node_conditions.go:123] node cpu capacity is 12
I0124 21:08:45.995114   25360 node_conditions.go:105] duration metric: took 93.657ms to run NodePressure ...
I0124 21:08:45.995114   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0124 21:08:49.996206   25360 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (4.0010921s)
I0124 21:08:49.996206   25360 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0124 21:08:50.483930   25360 ops.go:34] apiserver oom_adj: -16
I0124 21:08:50.484924   25360 kubeadm.go:640] restartCluster took 55.8952918s
I0124 21:08:50.484924   25360 kubeadm.go:406] StartCluster complete in 56.0303306s
I0124 21:08:50.484924   25360 settings.go:142] acquiring lock: {Name:mk6f111bd8882d34c65dffc2e09a2a566fce5b8c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0124 21:08:50.484924   25360 settings.go:150] Updating kubeconfig:  C:\Users\ZERO\.kube\config
I0124 21:08:50.488930   25360 lock.go:35] WriteFile acquiring C:\Users\ZERO\.kube\config: {Name:mk6a5748766e88e3a509e3a4f1708fba5743243f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0124 21:08:50.495924   25360 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0124 21:08:50.497923   25360 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:true inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0124 21:08:50.499936   25360 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0124 21:08:50.499936   25360 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0124 21:08:50.499936   25360 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0124 21:08:50.499936   25360 addons.go:69] Setting dashboard=true in profile "minikube"
I0124 21:08:50.499936   25360 addons.go:69] Setting ingress-dns=true in profile "minikube"
I0124 21:08:50.500932   25360 addons.go:69] Setting ingress=true in profile "minikube"
I0124 21:08:50.500932   25360 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0124 21:08:50.500932   25360 addons.go:240] addon storage-provisioner should already be in state true
I0124 21:08:50.500932   25360 addons.go:231] Setting addon ingress-dns=true in "minikube"
W0124 21:08:50.500932   25360 addons.go:240] addon ingress-dns should already be in state true
I0124 21:08:50.500932   25360 addons.go:231] Setting addon dashboard=true in "minikube"
W0124 21:08:50.500932   25360 addons.go:240] addon dashboard should already be in state true
I0124 21:08:50.501931   25360 addons.go:231] Setting addon ingress=true in "minikube"
W0124 21:08:50.501931   25360 addons.go:240] addon ingress should already be in state true
I0124 21:08:50.501931   25360 host.go:66] Checking if "minikube" exists ...
I0124 21:08:50.501931   25360 host.go:66] Checking if "minikube" exists ...
I0124 21:08:50.501931   25360 host.go:66] Checking if "minikube" exists ...
I0124 21:08:50.501931   25360 host.go:66] Checking if "minikube" exists ...
I0124 21:08:50.565967   25360 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0124 21:08:50.661555   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:08:50.679479   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:08:50.688408   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:08:50.700344   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:08:50.704536   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:08:51.396267   25360 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0124 21:08:51.399778   25360 out.go:177] üí°  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0124 21:08:51.400931   25360 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0124 21:08:51.375488   25360 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0124 21:08:51.400931   25360 addons.go:240] addon default-storageclass should already be in state true
I0124 21:08:51.402632   25360 out.go:177] üí°  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0124 21:08:51.403190   25360 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0124 21:08:51.403885   25360 host.go:66] Checking if "minikube" exists ...
I0124 21:08:51.406369   25360 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0124 21:08:51.405203   25360 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0124 21:08:51.450576   25360 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2
I0124 21:08:51.406953   25360 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0124 21:08:51.439578   25360 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0124 21:08:51.561127   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0124 21:08:51.450576   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0124 21:08:51.561127   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0124 21:08:51.598143   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:08:51.797176   25360 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0124 21:08:51.743431   25360 out.go:177] üîé  Verifying Kubernetes components...
I0124 21:08:51.795240   25360 addons.go:423] installing /etc/kubernetes/addons/ingress-dns-pod.yaml
I0124 21:08:51.796606   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:08:51.797176   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-dns-pod.yaml (2442 bytes)
I0124 21:08:51.847102   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:08:51.954291   25360 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0124 21:08:51.961794   25360 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/controller:v1.9.4
I0124 21:08:52.000444   25360 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0124 21:08:52.121303   25360 addons.go:423] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0124 21:08:52.121303   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16103 bytes)
I0124 21:08:52.147299   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:08:52.692829   25360 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.1317014s)
I0124 21:08:52.692829   25360 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0124 21:08:52.692829   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0124 21:08:52.707601   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0124 21:08:52.894200   25360 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.0970234s)
I0124 21:08:52.894200   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:08:52.910195   25360 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.1130188s)
I0124 21:08:52.910195   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:08:52.926274   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:08:52.926274   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:08:53.283993   25360 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57867 SSHKeyPath:C:\Users\ZERO\.minikube\machines\minikube\id_rsa Username:docker}
I0124 21:08:56.623362   25360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0124 21:08:56.820697   25360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0124 21:08:56.974597   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0124 21:08:56.974597   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0124 21:08:57.019674   25360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-dns-pod.yaml
I0124 21:08:57.021671   25360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0124 21:08:57.588094   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0124 21:08:57.588094   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0124 21:08:57.995989   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0124 21:08:57.995989   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0124 21:08:58.271093   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0124 21:08:58.271093   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0124 21:08:58.505080   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0124 21:08:58.506077   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0124 21:08:58.783316   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0124 21:08:58.783316   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0124 21:08:59.190955   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0124 21:08:59.190955   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0124 21:08:59.779184   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0124 21:08:59.779184   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0124 21:09:00.188276   25360 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0124 21:09:00.189278   25360 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0124 21:09:00.719306   25360 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0124 21:09:03.895718   25360 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (11.779425s)
I0124 21:09:03.895718   25360 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (13.3997943s)
I0124 21:09:03.895718   25360 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0124 21:09:03.921718   25360 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0124 21:09:04.235931   25360 api_server.go:52] waiting for apiserver process to appear ...
I0124 21:09:04.266931   25360 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0124 21:09:13.781134   25360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (17.1577721s)
I0124 21:09:16.996589   25360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-dns-pod.yaml: (19.9769155s)
I0124 21:09:16.996589   25360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (20.1758925s)
I0124 21:09:21.708006   25360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (24.6853335s)
I0124 21:09:21.708006   25360 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (20.9886995s)
I0124 21:09:21.708006   25360 addons.go:467] Verifying addon ingress=true in "minikube"
I0124 21:09:21.708006   25360 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (17.4410749s)
I0124 21:09:21.708006   25360 api_server.go:72] duration metric: took 30.1468782s to wait for apiserver process to appear ...
I0124 21:09:21.708006   25360 api_server.go:88] waiting for apiserver healthz status ...
I0124 21:09:21.708006   25360 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57871/healthz ...
I0124 21:09:21.710004   25360 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0124 21:09:21.709005   25360 out.go:177] üîé  Verifying ingress addon...
I0124 21:09:21.714007   25360 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0124 21:09:21.788874   25360 api_server.go:279] https://127.0.0.1:57871/healthz returned 200:
ok
I0124 21:09:21.789506   25360 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0124 21:09:21.789506   25360 kapi.go:107] duration metric: took 75.4989ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0124 21:09:21.792805   25360 out.go:177] üåü  Enabled addons: default-storageclass, ingress-dns, storage-provisioner, dashboard, ingress
I0124 21:09:21.796539   25360 addons.go:502] enable addons completed in 31.3016192s: enabled=[default-storageclass ingress-dns storage-provisioner dashboard ingress]
I0124 21:09:21.799922   25360 api_server.go:141] control plane version: v1.28.3
I0124 21:09:21.799922   25360 api_server.go:131] duration metric: took 91.9163ms to wait for apiserver health ...
I0124 21:09:21.799922   25360 system_pods.go:43] waiting for kube-system pods to appear ...
I0124 21:09:21.819066   25360 system_pods.go:59] 8 kube-system pods found
I0124 21:09:21.819066   25360 system_pods.go:61] "coredns-5dd5756b68-7hzfs" [62967fb2-ef09-4d0c-8ca8-a985ac49f72e] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "etcd-minikube" [92c60c16-f9d6-47a2-8a0c-3c01d5180229] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "kube-apiserver-minikube" [5a285dae-5a3d-4a1d-a97a-f2ad9bfcb64b] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "kube-controller-manager-minikube" [4a0c9971-e680-4c8b-a85a-e61af2abe6aa] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "kube-ingress-dns-minikube" [dc3765bd-98a3-4de5-80f2-a5e2b9071cab] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "kube-proxy-bkhn7" [aaf9c56e-0e80-4fe5-8056-b0df9b7c8e87] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "kube-scheduler-minikube" [be7b784c-3645-4132-b2f3-37b5f590a81f] Running
I0124 21:09:21.819066   25360 system_pods.go:61] "storage-provisioner" [3989d12b-fc52-4bd9-b6db-a97707ba6197] Running
I0124 21:09:21.819066   25360 system_pods.go:74] duration metric: took 19.1437ms to wait for pod list to return data ...
I0124 21:09:21.819066   25360 kubeadm.go:581] duration metric: took 30.2579382s to wait for : map[apiserver:true system_pods:true] ...
I0124 21:09:21.819066   25360 node_conditions.go:102] verifying NodePressure condition ...
I0124 21:09:21.877342   25360 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0124 21:09:21.877342   25360 node_conditions.go:123] node cpu capacity is 12
I0124 21:09:21.877342   25360 node_conditions.go:105] duration metric: took 58.2764ms to run NodePressure ...
I0124 21:09:21.877342   25360 start.go:228] waiting for startup goroutines ...
I0124 21:09:21.877342   25360 start.go:233] waiting for cluster config update ...
I0124 21:09:21.877342   25360 start.go:242] writing updated cluster config ...
I0124 21:09:21.927111   25360 ssh_runner.go:195] Run: rm -f paused
I0124 21:09:22.278669   25360 start.go:600] kubectl: 1.25.9, cluster: 1.28.3 (minor skew: 3)
I0124 21:09:22.279667   25360 out.go:177] 
W0124 21:09:22.280665   25360 out.go:239] ‚ùó  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.25.9, which may have incompatibilities with Kubernetes 1.28.3.
I0124 21:09:22.282662   25360 out.go:177]     ‚ñ™ Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I0124 21:09:22.285664   25360 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 28 02:59:03 minikube cri-dockerd[1279]: time="2024-01-28T02:59:03Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 02:59:17 minikube cri-dockerd[1279]: time="2024-01-28T02:59:17Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 02:59:34 minikube cri-dockerd[1279]: time="2024-01-28T02:59:34Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 02:59:49 minikube cri-dockerd[1279]: time="2024-01-28T02:59:49Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:00:06 minikube cri-dockerd[1279]: time="2024-01-28T03:00:06Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:00:20 minikube cri-dockerd[1279]: time="2024-01-28T03:00:20Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:00:35 minikube cri-dockerd[1279]: time="2024-01-28T03:00:35Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:00:48 minikube cri-dockerd[1279]: time="2024-01-28T03:00:48Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:01:04 minikube cri-dockerd[1279]: time="2024-01-28T03:01:04Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:01:16 minikube cri-dockerd[1279]: time="2024-01-28T03:01:16Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:01:32 minikube cri-dockerd[1279]: time="2024-01-28T03:01:32Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:01:47 minikube cri-dockerd[1279]: time="2024-01-28T03:01:47Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:02:03 minikube cri-dockerd[1279]: time="2024-01-28T03:02:03Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:02:19 minikube cri-dockerd[1279]: time="2024-01-28T03:02:19Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:02:35 minikube cri-dockerd[1279]: time="2024-01-28T03:02:35Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:02:51 minikube cri-dockerd[1279]: time="2024-01-28T03:02:51Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:03:07 minikube cri-dockerd[1279]: time="2024-01-28T03:03:07Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:03:20 minikube cri-dockerd[1279]: time="2024-01-28T03:03:20Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:03:32 minikube cri-dockerd[1279]: time="2024-01-28T03:03:32Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:03:45 minikube cri-dockerd[1279]: time="2024-01-28T03:03:45Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:03:57 minikube cri-dockerd[1279]: time="2024-01-28T03:03:57Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:04:13 minikube cri-dockerd[1279]: time="2024-01-28T03:04:13Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:04:25 minikube cri-dockerd[1279]: time="2024-01-28T03:04:25Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:04:38 minikube cri-dockerd[1279]: time="2024-01-28T03:04:38Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:04:50 minikube cri-dockerd[1279]: time="2024-01-28T03:04:50Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:05:06 minikube cri-dockerd[1279]: time="2024-01-28T03:05:06Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:05:18 minikube cri-dockerd[1279]: time="2024-01-28T03:05:18Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:05:31 minikube cri-dockerd[1279]: time="2024-01-28T03:05:31Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:05:47 minikube cri-dockerd[1279]: time="2024-01-28T03:05:47Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:06:01 minikube cri-dockerd[1279]: time="2024-01-28T03:06:01Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:06:16 minikube cri-dockerd[1279]: time="2024-01-28T03:06:16Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:06:32 minikube cri-dockerd[1279]: time="2024-01-28T03:06:32Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:06:48 minikube cri-dockerd[1279]: time="2024-01-28T03:06:48Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:07:04 minikube cri-dockerd[1279]: time="2024-01-28T03:07:04Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:07:17 minikube cri-dockerd[1279]: time="2024-01-28T03:07:17Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:07:33 minikube cri-dockerd[1279]: time="2024-01-28T03:07:33Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:07:46 minikube cri-dockerd[1279]: time="2024-01-28T03:07:46Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:08:01 minikube cri-dockerd[1279]: time="2024-01-28T03:08:01Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:08:14 minikube cri-dockerd[1279]: time="2024-01-28T03:08:14Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:08:30 minikube cri-dockerd[1279]: time="2024-01-28T03:08:30Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:08:44 minikube cri-dockerd[1279]: time="2024-01-28T03:08:44Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:08:57 minikube cri-dockerd[1279]: time="2024-01-28T03:08:57Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:09:12 minikube cri-dockerd[1279]: time="2024-01-28T03:09:12Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:09:25 minikube cri-dockerd[1279]: time="2024-01-28T03:09:25Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:09:41 minikube cri-dockerd[1279]: time="2024-01-28T03:09:41Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:09:56 minikube cri-dockerd[1279]: time="2024-01-28T03:09:56Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:10:12 minikube cri-dockerd[1279]: time="2024-01-28T03:10:12Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:10:26 minikube cri-dockerd[1279]: time="2024-01-28T03:10:26Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:10:38 minikube cri-dockerd[1279]: time="2024-01-28T03:10:38Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:10:50 minikube cri-dockerd[1279]: time="2024-01-28T03:10:50Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:11:02 minikube cri-dockerd[1279]: time="2024-01-28T03:11:02Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:11:18 minikube cri-dockerd[1279]: time="2024-01-28T03:11:18Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:11:34 minikube cri-dockerd[1279]: time="2024-01-28T03:11:34Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:11:47 minikube cri-dockerd[1279]: time="2024-01-28T03:11:47Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:12:00 minikube cri-dockerd[1279]: time="2024-01-28T03:12:00Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:12:16 minikube cri-dockerd[1279]: time="2024-01-28T03:12:16Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:12:30 minikube cri-dockerd[1279]: time="2024-01-28T03:12:30Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:12:43 minikube cri-dockerd[1279]: time="2024-01-28T03:12:43Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:12:55 minikube cri-dockerd[1279]: time="2024-01-28T03:12:55Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"
Jan 28 03:13:08 minikube cri-dockerd[1279]: time="2024-01-28T03:13:08Z" level=info msg="Stop pulling image briyankessel/site_k8s_project:latest: Status: Image is up to date for briyankessel/site_k8s_project:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
d5845e597f2fa       c1e2daef1e6e1                                                                                                                2 days ago          Running             mysql                       3                   3d5c61bbb023e       mysql-tp-0
0fbf552fcd618       6e38f40d628db                                                                                                                2 days ago          Running             storage-provisioner         8                   8237f3f73c011       storage-provisioner
62703002a9ebb       6e38f40d628db                                                                                                                2 days ago          Exited              storage-provisioner         7                   8237f3f73c011       storage-provisioner
1952a0cec6fbb       c1e2daef1e6e1                                                                                                                2 days ago          Exited              mysql                       2                   3d5c61bbb023e       mysql-tp-0
2a8f89864f489       briyankessel/api@sha256:42598386d558c359a597f1e158ad5de189aae19d73709120168231c5d53cb964                                     3 days ago          Running             api                         0                   23885e5fc8b6c       api-7586cdbcd4-9wzdm
60778a244ee16       07655ddf2eebe                                                                                                                3 days ago          Running             kubernetes-dashboard        4                   5086c3f8a7395       kubernetes-dashboard-8694d4445c-lwgc4
125492a6a7390       efrei2023/frontend@sha256:eb72237fa556a5e04014ead9e7fc95ebabbcdf1da1f42bb791810e7f5bb19a3a                                   3 days ago          Running             frontend                    1                   328393c7749f7       frontend-7c849d7bfd-n9c8n
24a7bb7145758       efrei2023/backend@sha256:424f3ca0d0aae5d0f42a73e65482a67ea145533f7619d3b40077bb6249703dc6                                    3 days ago          Running             backend-container           1                   d3941ec4d6cc6       backend-57cd8d587-gw2f2
01acd9e3b08f7       5aa0bf4798fa2                                                                                                                3 days ago          Running             controller                  2                   29d69f88b28da       ingress-nginx-controller-7c6974c4d8-nrfwg
9499d036aa331       07655ddf2eebe                                                                                                                3 days ago          Exited              kubernetes-dashboard        3                   5086c3f8a7395       kubernetes-dashboard-8694d4445c-lwgc4
0ef02df6403a6       ead0a4a53df89                                                                                                                3 days ago          Running             coredns                     2                   2635018acad25       coredns-5dd5756b68-7hzfs
252a4ca2ab1b5       115053965e86b                                                                                                                3 days ago          Running             dashboard-metrics-scraper   2                   40695ec243a25       dashboard-metrics-scraper-7fd5cb4ddc-x5pg2
b6670dcba77a2       04313d9950fee                                                                                                                3 days ago          Running             myservice                   2                   cf1b9eef32181       myservice2-bb48c65-tl4hz
a16a6318c70d0       1499ed4fbd0aa                                                                                                                3 days ago          Running             minikube-ingress-dns        2                   a7a78048a3d74       kube-ingress-dns-minikube
86cb1dc6236ec       bfc896cf80fba                                                                                                                3 days ago          Running             kube-proxy                  2                   d5b1687b018b3       kube-proxy-bkhn7
919612816b57b       5374347291230                                                                                                                3 days ago          Running             kube-apiserver              2                   ab62af5fc354e       kube-apiserver-minikube
e4d398a9621b6       73deb9a3f7025                                                                                                                3 days ago          Running             etcd                        2                   514bdbf1464bb       etcd-minikube
97481c61de5ad       10baa1ca17068                                                                                                                3 days ago          Running             kube-controller-manager     2                   187b15c9f91b8       kube-controller-manager-minikube
029e6818a6d56       6d1b4fd1b182d                                                                                                                3 days ago          Running             kube-scheduler              2                   33ad5ddc6a8b6       kube-scheduler-minikube
6314e438ef51b       efrei2023/frontend@sha256:eb72237fa556a5e04014ead9e7fc95ebabbcdf1da1f42bb791810e7f5bb19a3a                                   10 days ago         Exited              frontend                    0                   124a508b27963       frontend-7c849d7bfd-n9c8n
aec9988bd76d0       efrei2023/backend@sha256:424f3ca0d0aae5d0f42a73e65482a67ea145533f7619d3b40077bb6249703dc6                                    10 days ago         Exited              backend-container           0                   75e7148c471c5       backend-57cd8d587-gw2f2
bedb9f733a19c       5aa0bf4798fa2                                                                                                                10 days ago         Exited              controller                  1                   6e60a91ceac64       ingress-nginx-controller-7c6974c4d8-nrfwg
52f3934ebca44       04313d9950fee                                                                                                                10 days ago         Exited              myservice                   1                   2c56ef94491cc       myservice2-bb48c65-tl4hz
357be60d8238c       ead0a4a53df89                                                                                                                10 days ago         Exited              coredns                     1                   d02b56ae72fa8       coredns-5dd5756b68-7hzfs
9ba34dab083a8       115053965e86b                                                                                                                10 days ago         Exited              dashboard-metrics-scraper   1                   0e4d654b9a2b4       dashboard-metrics-scraper-7fd5cb4ddc-x5pg2
392beaeff1bc9       1499ed4fbd0aa                                                                                                                10 days ago         Exited              minikube-ingress-dns        1                   2a9aacf194e85       kube-ingress-dns-minikube
044a778148bc5       bfc896cf80fba                                                                                                                10 days ago         Exited              kube-proxy                  1                   b97e6926bf8b3       kube-proxy-bkhn7
618a2c0ae0505       10baa1ca17068                                                                                                                10 days ago         Exited              kube-controller-manager     1                   00c4e68c561b1       kube-controller-manager-minikube
41c74e564dfff       5374347291230                                                                                                                10 days ago         Exited              kube-apiserver              1                   397560b4b935f       kube-apiserver-minikube
60e07ea32bc4e       73deb9a3f7025                                                                                                                10 days ago         Exited              etcd                        1                   e300604e0bf64       etcd-minikube
7c140e392dac7       6d1b4fd1b182d                                                                                                                10 days ago         Exited              kube-scheduler              1                   f8e0fb158fafd       kube-scheduler-minikube
6bf49e0cd8a49       1ebff0f9671bc                                                                                                                2 months ago        Exited              patch                       1                   97dff04909286       ingress-nginx-admission-patch-grp9h
99e766f466ddf       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a7943503b45d552785aa3b5e457f169a5661fb94d82b8a3373bcd9ebaf9aac80   2 months ago        Exited              create                      0                   f17ef65b2d163       ingress-nginx-admission-create-49r5z

* 
* ==> controller_ingress [01acd9e3b08f] <==
* I0124 20:09:12.375366       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"0f5dc6c5-4428-46c4-b8ec-ebda68357a97", APIVersion:"networking.k8s.io/v1", ResourceVersion:"67228", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0124 20:09:12.471523       7 nginx.go:303] "Starting NGINX process"
I0124 20:09:12.472270       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0124 20:09:12.472717       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
W0124 20:09:12.480116       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
I0124 20:09:12.481016       7 controller.go:190] "Configuration changes detected, backend reload required"
I0124 20:09:12.871394       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0124 20:09:12.871858       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-nrfwg"
I0124 20:09:12.976278       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-nrfwg" node="minikube"
I0124 20:09:13.177253       7 status.go:304] "updating Ingress status" namespace="default" ingress="example-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0124 20:09:13.570708       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"0f5dc6c5-4428-46c4-b8ec-ebda68357a97", APIVersion:"networking.k8s.io/v1", ResourceVersion:"77532", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0124 20:09:15.572181       7 controller.go:210] "Backend successfully reloaded"
I0124 20:09:15.572389       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0124 20:09:15.572512       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-nrfwg", UID:"3be63a31-26ff-4dff-a737-4ae12c8ec59c", APIVersion:"v1", ResourceVersion:"77466", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0124 20:09:16.577448       7 controller.go:241] Dynamic reconfiguration failed (retrying; 15 retries left): Post "http://127.0.0.1:10246/configuration/backends": dial tcp 127.0.0.1:10246: connect: connection refused
W0124 20:09:20.877678       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:09:24.269476       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:09:27.568226       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:09:30.878083       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:09:34.213924       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:09:55.186007       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:09:58.522876       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
I0124 20:10:13.432942       7 status.go:304] "updating Ingress status" namespace="default" ingress="example-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0124 20:10:13.456995       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"0f5dc6c5-4428-46c4-b8ec-ebda68357a97", APIVersion:"networking.k8s.io/v1", ResourceVersion:"77629", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W0124 20:10:13.457482       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0124 20:48:46.814151       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:41:57.586409       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:01.969195       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:03.970094       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:07.070620       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:10.406806       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:13.739240       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:15.969654       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 08:42:45.071445       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 12:55:58.782084       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 12:56:02.070027       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 12:56:05.344674       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 12:56:10.647041       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 12:56:13.981190       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 12:56:17.314364       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
E0125 19:00:30.409380       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0125 19:00:30.813015       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0125 19:00:31.113393       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0125 19:00:32.210227       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0125 19:00:41.412129       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
W0125 19:00:42.901604       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:00:46.493493       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:00:49.493942       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:00:52.811143       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:00:56.293314       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:00:59.474975       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:01:02.808192       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:01:06.141070       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0125 19:01:44.802813       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0126 16:08:38.082956       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0126 16:16:13.537812       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0126 16:23:58.088336       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0126 16:26:18.080118       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0128 02:54:08.950281       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0128 02:57:37.025088       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store

* 
* ==> controller_ingress [bedb9f733a19] <==
* W0117 11:04:41.269553       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0117 11:04:41.270076       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0117 11:04:41.468371       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0117 11:04:42.963645       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0117 11:04:43.158146       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0117 11:04:43.175422       7 nginx.go:260] "Starting NGINX Ingress controller"
I0117 11:04:43.182123       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"fb66d394-77f4-40e8-be00-9e55ddc69735", APIVersion:"v1", ResourceVersion:"6751", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0117 11:04:43.261714       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"973e31d3-3cf5-43c1-854b-fb43223e950f", APIVersion:"v1", ResourceVersion:"6752", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0117 11:04:43.261786       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"d7f490e7-b07a-4da1-ba49-329d5f2802c1", APIVersion:"v1", ResourceVersion:"6753", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0117 11:04:44.280330       7 store.go:440] "Found valid IngressClass" ingress="default/example-ingress" ingressclass="nginx"
I0117 11:04:44.281011       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"0f5dc6c5-4428-46c4-b8ec-ebda68357a97", APIVersion:"networking.k8s.io/v1", ResourceVersion:"7323", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0117 11:04:44.378382       7 nginx.go:303] "Starting NGINX process"
I0117 11:04:44.459176       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0117 11:04:44.459311       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
W0117 11:04:44.467757       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
I0117 11:04:44.468045       7 controller.go:190] "Configuration changes detected, backend reload required"
I0117 11:04:44.472902       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0117 11:04:44.472962       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-nrfwg"
I0117 11:04:44.562884       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-nrfwg" node="minikube"
I0117 11:04:44.567076       7 status.go:304] "updating Ingress status" namespace="default" ingress="example-ingress" currentValue=[{"ip":"192.168.49.2"}] newValue=[]
I0117 11:04:44.663270       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"0f5dc6c5-4428-46c4-b8ec-ebda68357a97", APIVersion:"networking.k8s.io/v1", ResourceVersion:"67141", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0117 11:04:45.177753       7 controller.go:210] "Backend successfully reloaded"
I0117 11:04:45.178002       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0117 11:04:45.178156       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-nrfwg", UID:"3be63a31-26ff-4dff-a737-4ae12c8ec59c", APIVersion:"v1", ResourceVersion:"67128", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0117 11:04:48.237940       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:04:51.572413       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:04:54.906492       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:04:58.263372       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:05:08.697735       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
I0117 11:05:44.485267       7 status.go:304] "updating Ingress status" namespace="default" ingress="example-ingress" currentValue=null newValue=[{"ip":"192.168.49.2"}]
I0117 11:05:44.491655       7 event.go:298] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"0f5dc6c5-4428-46c4-b8ec-ebda68357a97", APIVersion:"networking.k8s.io/v1", ResourceVersion:"67228", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W0117 11:05:44.491700       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:22:36.317378       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:25:43.889878       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:25:47.224283       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:26:40.699837       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:33:23.564172       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:33:26.898311       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:49:36.354930       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:52:21.004585       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:52:24.338712       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:52:51.144360       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:52:54.478138       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 11:52:57.813922       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 12:00:19.120285       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 12:01:01.321247       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 12:17:00.035139       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 12:17:03.368704       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 12:17:28.423026       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
W0117 12:17:52.095790       7 controller.go:1108] Error obtaining Endpoints for Service "default/myservice": no object matching key "default/myservice" in local store
E0117 16:10:22.359883       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0117 16:10:22.367562       7 leaderelection.go:280] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0117 16:10:22.369604       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0117 16:10:22.459926       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0117 16:10:23.262025       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------


* 
* ==> coredns [0ef02df6403a] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:38630 - 5107 "HINFO IN 6832168191956696373.3128706803913782914. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.295930258s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.610170094s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.148037855s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.EndpointSlice ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Service ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: watch of *v1.Namespace ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.460590413s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.200679306s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.016599337s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.678843398s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

* 
* ==> coredns [357be60d8238] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:48691 - 20041 "HINFO IN 6099057847881375238.3854458053124382111. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.091985813s
[INFO] 10.244.0.23:58505 - 23564 "AAAA IN backend.default.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.002009946s
[INFO] 10.244.0.23:58505 - 12811 "A IN backend.default.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.002250221s
[INFO] 10.244.0.23:55517 - 51462 "AAAA IN backend.svc.cluster.local. udp 43 false 512" NXDOMAIN qr,aa,rd 136 0.00028151s
[INFO] 10.244.0.23:55517 - 52994 "A IN backend.svc.cluster.local. udp 43 false 512" NXDOMAIN qr,aa,rd 136 0.000334157s
[INFO] 10.244.0.23:48139 - 60264 "A IN backend.cluster.local. udp 39 false 512" NXDOMAIN qr,aa,rd 132 0.000315313s
[INFO] 10.244.0.23:48139 - 23668 "AAAA IN backend.cluster.local. udp 39 false 512" NXDOMAIN qr,aa,rd 132 0.000696818s
[INFO] 10.244.0.23:33755 - 7564 "AAAA IN backend. udp 25 false 512" - - 0 6.002500073s
[ERROR] plugin/errors: 2 backend. AAAA: read udp 10.244.0.14:48610->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.23:33755 - 62599 "A IN backend. udp 25 false 512" - - 0 6.002542063s
[ERROR] plugin/errors: 2 backend. A: read udp 10.244.0.14:48776->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.23:33755 - 7564 "AAAA IN backend. udp 25 false 512" - - 0 6.002825544s
[ERROR] plugin/errors: 2 backend. AAAA: read udp 10.244.0.14:41207->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.23:33755 - 62599 "A IN backend. udp 25 false 512" - - 0 6.003041667s
[ERROR] plugin/errors: 2 backend. A: read udp 10.244.0.14:41974->192.168.65.254:53: i/o timeout

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_14T19_47_52_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 14 Nov 2023 18:47:49 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Jan 2024 03:13:12 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Jan 2024 03:08:15 +0000   Thu, 25 Jan 2024 19:00:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Jan 2024 03:08:15 +0000   Thu, 25 Jan 2024 19:00:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Jan 2024 03:08:15 +0000   Thu, 25 Jan 2024 19:00:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Jan 2024 03:08:15 +0000   Thu, 25 Jan 2024 19:00:56 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7834776Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7834776Ki
  pods:               110
System Info:
  Machine ID:                 8b1e4f572b7b4181a82c9036944292d0
  System UUID:                8b1e4f572b7b4181a82c9036944292d0
  Boot ID:                    1bfb5a97-e25f-45ff-83c0-5caeee432caa
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     api-5ffb9d968f-65972                          500m (4%!)(MISSING)     1 (8%!)(MISSING)      256Mi (3%!)(MISSING)       512Mi (6%!)(MISSING)     35h
  default                     api-7586cdbcd4-9wzdm                          500m (4%!)(MISSING)     1 (8%!)(MISSING)      256Mi (3%!)(MISSING)       512Mi (6%!)(MISSING)     3d6h
  default                     backend-57cd8d587-gw2f2                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  default                     frontend-7c849d7bfd-n9c8n                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  default                     myservice2-bb48c65-tl4hz                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  default                     site-55857bc65-tcsfg                          500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     34h
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-nrfwg     100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (1%!)(MISSING)        0 (0%!)(MISSING)         74d
  kube-system                 coredns-5dd5756b68-7hzfs                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     74d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         74d
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kube-system                 kube-ingress-dns-minikube                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kube-system                 kube-proxy-bkhn7                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-x5pg2    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-lwgc4         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         74d
  prod                        mysql-tp-0                                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                2350m (19%!)(MISSING)  2500m (20%!)(MISSING)
  memory             900Mi (11%!)(MISSING)  1322Mi (17%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.012146] Code: 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa 41 89 ca 64 8b 04 25 18 00 00 00 85 c0 75 15 b8 e8 00 00 00 0f 05 <48> 3d 00 f0 ff ff 77 5e c3 0f 1f 44 00 00 48 83 ec 28 89 54 24 18
[  +0.000013] RSP: 002b:00007fffe4d33ea8 EFLAGS: 00000246 ORIG_RAX: 00000000000000e8
[  +0.000054] RAX: 0000000000000002 RBX: 000055ca23da2cd0 RCX: 00007fc8a4eddfda
[  +0.000002] RDX: 0000000000000028 RSI: 000055ca23da9be0 RDI: 000000000000000d
[  +0.000003] RBP: 0000000000136d7f R08: 0000000000000028 R09: 0000000065b24932
[  +0.000002] R10: 00000000ffffffff R11: 0000000000000246 R12: 00000000000000c8
[  +0.000001] R13: 0000000000000028 R14: 0000000000000014 R15: 000055ca23da2e60
[  +0.000002] FS:  00007fc8a44fe900 GS:  0000000000000000
[ +13.935804] systemd-journald[1598216]: File /run/log/journal/8b1e4f572b7b4181a82c9036944292d0/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jan25 12:55] hv_utils: TIMESYNC IC: Stale time stamp, 2139627197700 nsecs old
[  +0.007546] CPU: 8 PID: 1706737 Comm: systemd-journal Not tainted 5.15.133.1-microsoft-standard-WSL2 #1
[  +0.000007] RIP: 0033:0x7f4285aa5fda
[  +0.000029] Code: 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa 41 89 ca 64 8b 04 25 18 00 00 00 85 c0 75 15 b8 e8 00 00 00 0f 05 <48> 3d 00 f0 ff ff 77 5e c3 0f 1f 44 00 00 48 83 ec 28 89 54 24 18
[  +0.000004] RSP: 002b:00007ffd07fcc4b8 EFLAGS: 00000246 ORIG_RAX: 00000000000000e8
[  +0.021779] RAX: 0000000000000002 RBX: 000055907a1e1cd0 RCX: 00007f4285aa5fda
[  +0.000003] RDX: 0000000000000028 RSI: 000055907a1e8c20 RDI: 000000000000000d
[  +0.000002] RBP: 0000000000186308 R08: 0000000000000028 R09: 0000000065b251dd
[  +0.000001] R10: 00000000ffffffff R11: 0000000000000246 R12: 00000000000000c8
[  +0.000002] R13: 0000000000000028 R14: 0000000000000014 R15: 000055907a1e1e60
[  +0.000001] FS:  00007f42850c6900 GS:  0000000000000000
[ +14.791311] systemd-journald[1618944]: File /run/log/journal/8b1e4f572b7b4181a82c9036944292d0/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jan25 19:00] rcu: INFO: rcu_sched self-detected stall on CPU
[  +0.717045] rcu: 	1-...0: (1 GPs behind) idle=a33/0/0x1 softirq=1407177/1407178 fqs=4 
[  +0.007384] 	(t=1006609 jiffies g=4256589 q=114)
[  +0.014555] NMI backtrace for cpu 1
[  +0.000102] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.15.133.1-microsoft-standard-WSL2 #1
[  +0.002289] Call Trace:
[  +0.003070]  <IRQ>
[  +0.003011]  dump_stack_lvl+0x34/0x48
[  +0.007588]  nmi_cpu_backtrace.cold+0x30/0x70
[  +0.000005]  ? lapic_can_unplug_cpu+0x80/0x80
[  +0.005411]  nmi_trigger_cpumask_backtrace+0xcd/0xd0
[  +0.007896]  rcu_dump_cpu_stacks+0xc1/0xf3
[  +0.000071]  rcu_sched_clock_irq.cold+0xe8/0x220
[  +0.000005]  update_process_times+0x8c/0xc0
[  +0.002049]  tick_sched_timer+0x8c/0xa0
[  +0.000105]  ? tick_sched_do_timer+0x90/0x90
[  +0.000004]  __hrtimer_run_queues+0x124/0x270
[  +0.000004]  hrtimer_interrupt+0x10e/0x240
[  +0.000002]  __sysvec_hyperv_stimer0+0x2e/0x60
[  +0.000044]  sysvec_hyperv_stimer0+0x6d/0x90
[  +0.002081]  </IRQ>
[  +0.000003]  <TASK>
[  +0.000252]  asm_sysvec_hyperv_stimer0+0x16/0x20
[  +0.000094] RIP: 0010:default_idle+0x10/0x20
[  +0.000009] Code: 56 a5 2c 00 0f ae f0 0f ae 38 0f ae f0 eb b5 66 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 eb 07 0f 00 2d 02 d5 4d 00 fb f4 <e9> 2b a5 2c 00 cc cc cc cc cc cc cc cc cc cc cc 0f 1f 44 00 00 65
[  +0.000025] RSP: 0018:ffffad4d400abef8 EFLAGS: 00000202
[  +0.000028] RAX: ffffffff8cf3ae00 RBX: ffff9e67802c6e40 RCX: 0000000000000000
[  +0.000002] RDX: 0000000004b37a31 RSI: ffffad4d400abe88 RDI: 0000000004b37a32
[  +0.000001] RBP: 0000000000000001 R08: 0000000000000001 R09: 0000000000000000
[  +0.000001] R10: 0000000000000000 R11: 0000000000000004 R12: 0000000000000000
[  +0.000001] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
[  +0.000001]  ? mwait_idle+0x80/0x80
[  +0.001625]  ? mwait_idle+0x80/0x80
[  +0.000021]  default_idle_call+0x33/0xb0
[  +0.000004]  do_idle+0x1ea/0x220
[  +0.001852]  cpu_startup_entry+0x19/0x20
[  +0.000005]  secondary_startup_64_no_verify+0xb0/0xbb
[  +0.000458]  </TASK>
[  +0.002860] NMI backtrace for cpu 2 skipped: idling at default_idle+0x10/0x20

* 
* ==> etcd [60e07ea32bc4] <==
* {"level":"warn","ts":"2024-01-17T16:10:22.767397Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"23.206585516s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-17T16:10:22.767487Z","caller":"traceutil/trace.go:171","msg":"trace[1134843038] range","detail":"{range_begin:/registry/certificatesigningrequests/; range_end:/registry/certificatesigningrequests0; response_count:0; response_revision:77305; }","duration":"23.206687199s","start":"2024-01-17T16:09:59.559166Z","end":"2024-01-17T16:10:22.767459Z","steps":["trace[1134843038] 'agreement among raft nodes before linearized reading'  (duration: 23.206533703s)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.76765Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:09:59.559111Z","time spent":"23.206839609s","remote":"127.0.0.1:59836","response type":"/etcdserverpb.KV/Range","request count":0,"request size":80,"response count":0,"response size":30,"request content":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.79599Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"526.066734ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.796112Z","caller":"traceutil/trace.go:171","msg":"trace[1751625206] range","detail":"{range_begin:/registry/persistentvolumes/; range_end:/registry/persistentvolumes0; response_count:0; response_revision:77306; }","duration":"526.209072ms","start":"2024-01-17T16:10:22.269875Z","end":"2024-01-17T16:10:22.796084Z","steps":["trace[1751625206] 'agreement among raft nodes before linearized reading'  (duration: 525.975689ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.79628Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.26985Z","time spent":"526.394178ms","remote":"127.0.0.1:59734","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":1,"response size":32,"request content":"key:\"/registry/persistentvolumes/\" range_end:\"/registry/persistentvolumes0\" count_only:true "}
{"level":"info","ts":"2024-01-17T16:10:22.796963Z","caller":"traceutil/trace.go:171","msg":"trace[496897539] transaction","detail":"{read_only:false; response_revision:77306; number_of_response:1; }","duration":"137.099307ms","start":"2024-01-17T16:10:22.659831Z","end":"2024-01-17T16:10:22.79693Z","steps":["trace[496897539] 'process raft request'  (duration: 135.819101ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.797424Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"234.086793ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" ","response":"range_response_count:1 size:502"}
{"level":"info","ts":"2024-01-17T16:10:22.797507Z","caller":"traceutil/trace.go:171","msg":"trace[1081560341] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:77306; }","duration":"234.175695ms","start":"2024-01-17T16:10:22.563308Z","end":"2024-01-17T16:10:22.797484Z","steps":["trace[1081560341] 'agreement among raft nodes before linearized reading'  (duration: 234.010841ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.797865Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"511.199796ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-17T16:10:22.797935Z","caller":"traceutil/trace.go:171","msg":"trace[863837839] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:77306; }","duration":"511.28333ms","start":"2024-01-17T16:10:22.286633Z","end":"2024-01-17T16:10:22.797916Z","steps":["trace[863837839] 'agreement among raft nodes before linearized reading'  (duration: 511.156737ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.798008Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.2866Z","time spent":"511.379122ms","remote":"127.0.0.1:59620","response type":"/etcdserverpb.KV/Range","request count":0,"request size":120,"response count":0,"response size":30,"request content":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.798455Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"422.426632ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.798533Z","caller":"traceutil/trace.go:171","msg":"trace[26709229] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:77306; }","duration":"422.515134ms","start":"2024-01-17T16:10:22.375996Z","end":"2024-01-17T16:10:22.798511Z","steps":["trace[26709229] 'agreement among raft nodes before linearized reading'  (duration: 422.369902ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.798612Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.375884Z","time spent":"422.704736ms","remote":"127.0.0.1:59690","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":20,"response size":32,"request content":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.859709Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"579.388829ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.859829Z","caller":"traceutil/trace.go:171","msg":"trace[1563191783] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:77306; }","duration":"579.537015ms","start":"2024-01-17T16:10:22.280264Z","end":"2024-01-17T16:10:22.859801Z","steps":["trace[1563191783] 'agreement among raft nodes before linearized reading'  (duration: 579.30224ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.859931Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.280174Z","time spent":"579.728041ms","remote":"127.0.0.1:59876","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":32,"request content":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.860524Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"490.6692ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:890"}
{"level":"info","ts":"2024-01-17T16:10:22.860637Z","caller":"traceutil/trace.go:171","msg":"trace[537600546] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:77306; }","duration":"490.855397ms","start":"2024-01-17T16:10:22.369751Z","end":"2024-01-17T16:10:22.860606Z","steps":["trace[537600546] 'agreement among raft nodes before linearized reading'  (duration: 490.580799ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.860715Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.36973Z","time spent":"490.955387ms","remote":"127.0.0.1:59752","response type":"/etcdserverpb.KV/Range","request count":0,"request size":77,"response count":1,"response size":914,"request content":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" "}
{"level":"warn","ts":"2024-01-17T16:10:22.861049Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"577.813807ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.8611Z","caller":"traceutil/trace.go:171","msg":"trace[1083600840] range","detail":"{range_begin:/registry/controllerrevisions/; range_end:/registry/controllerrevisions0; response_count:0; response_revision:77306; }","duration":"577.873432ms","start":"2024-01-17T16:10:22.283211Z","end":"2024-01-17T16:10:22.861084Z","steps":["trace[1083600840] 'agreement among raft nodes before linearized reading'  (duration: 577.769876ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.861147Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.283198Z","time spent":"577.936012ms","remote":"127.0.0.1:60064","response type":"/etcdserverpb.KV/Range","request count":0,"request size":66,"response count":2,"response size":32,"request content":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.861642Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"580.900785ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2024-01-17T16:10:22.861679Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"581.206259ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.861718Z","caller":"traceutil/trace.go:171","msg":"trace[1129359140] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:77306; }","duration":"580.993373ms","start":"2024-01-17T16:10:22.280705Z","end":"2024-01-17T16:10:22.861698Z","steps":["trace[1129359140] 'agreement among raft nodes before linearized reading'  (duration: 580.844795ms)"],"step_count":1}
{"level":"info","ts":"2024-01-17T16:10:22.861769Z","caller":"traceutil/trace.go:171","msg":"trace[1012405803] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:77306; }","duration":"581.300038ms","start":"2024-01-17T16:10:22.280439Z","end":"2024-01-17T16:10:22.861739Z","steps":["trace[1012405803] 'agreement among raft nodes before linearized reading'  (duration: 581.152473ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.861784Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.280696Z","time spent":"581.072169ms","remote":"127.0.0.1:59682","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":7,"response size":32,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.861834Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.280417Z","time spent":"581.397124ms","remote":"127.0.0.1:60042","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":1,"response size":32,"request content":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.862141Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"581.499167ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-17T16:10:22.862215Z","caller":"traceutil/trace.go:171","msg":"trace[1824207592] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:0; response_revision:77306; }","duration":"581.574166ms","start":"2024-01-17T16:10:22.28062Z","end":"2024-01-17T16:10:22.862194Z","steps":["trace[1824207592] 'agreement among raft nodes before linearized reading'  (duration: 581.459052ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.862275Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.280604Z","time spent":"581.650789ms","remote":"127.0.0.1:59594","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":0,"response size":30,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2024-01-17T16:10:22.862729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"582.144959ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.862823Z","caller":"traceutil/trace.go:171","msg":"trace[779777942] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:77306; }","duration":"582.2458ms","start":"2024-01-17T16:10:22.280551Z","end":"2024-01-17T16:10:22.862797Z","steps":["trace[779777942] 'agreement among raft nodes before linearized reading'  (duration: 582.047873ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.862884Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.280541Z","time spent":"582.325388ms","remote":"127.0.0.1:60078","response type":"/etcdserverpb.KV/Range","request count":0,"request size":90,"response count":1,"response size":32,"request content":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.863037Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"588.29419ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.863109Z","caller":"traceutil/trace.go:171","msg":"trace[1751494463] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:77306; }","duration":"588.378866ms","start":"2024-01-17T16:10:22.274711Z","end":"2024-01-17T16:10:22.86309Z","steps":["trace[1751494463] 'agreement among raft nodes before linearized reading'  (duration: 588.248487ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.863234Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.274679Z","time spent":"588.468999ms","remote":"127.0.0.1:59920","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":2,"response size":32,"request content":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true "}
{"level":"warn","ts":"2024-01-17T16:10:22.863678Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"593.445782ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-17T16:10:22.863752Z","caller":"traceutil/trace.go:171","msg":"trace[22826245] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:77306; }","duration":"593.531529ms","start":"2024-01-17T16:10:22.270201Z","end":"2024-01-17T16:10:22.863733Z","steps":["trace[22826245] 'agreement among raft nodes before linearized reading'  (duration: 593.383222ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:22.863811Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-17T16:10:22.270179Z","time spent":"593.614091ms","remote":"127.0.0.1:59996","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":32,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"info","ts":"2024-01-17T16:10:23.072383Z","caller":"traceutil/trace.go:171","msg":"trace[1290528756] linearizableReadLoop","detail":"{readStateIndex:94201; appliedIndex:94200; }","duration":"112.426552ms","start":"2024-01-17T16:10:22.95992Z","end":"2024-01-17T16:10:23.072347Z","steps":["trace[1290528756] 'read index received'  (duration: 16.430482ms)","trace[1290528756] 'applied index is now lower than readState.Index'  (duration: 95.994518ms)"],"step_count":2}
{"level":"info","ts":"2024-01-17T16:10:23.072473Z","caller":"traceutil/trace.go:171","msg":"trace[1561121679] transaction","detail":"{read_only:false; response_revision:77307; number_of_response:1; }","duration":"200.286393ms","start":"2024-01-17T16:10:22.872111Z","end":"2024-01-17T16:10:23.072397Z","steps":["trace[1561121679] 'process raft request'  (duration: 199.998607ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:23.072656Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.760737ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-x5pg2\" ","response":"range_response_count:1 size:4022"}
{"level":"info","ts":"2024-01-17T16:10:23.072712Z","caller":"traceutil/trace.go:171","msg":"trace[1749177263] range","detail":"{range_begin:/registry/pods/kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-x5pg2; range_end:; response_count:1; response_revision:77307; }","duration":"112.844581ms","start":"2024-01-17T16:10:22.959851Z","end":"2024-01-17T16:10:23.072696Z","steps":["trace[1749177263] 'agreement among raft nodes before linearized reading'  (duration: 112.636899ms)"],"step_count":1}
WARNING: 2024/01/17 16:10:23 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"info","ts":"2024-01-17T16:10:23.181136Z","caller":"traceutil/trace.go:171","msg":"trace[817219744] transaction","detail":"{read_only:false; response_revision:77308; number_of_response:1; }","duration":"118.957908ms","start":"2024-01-17T16:10:23.062154Z","end":"2024-01-17T16:10:23.181112Z","steps":["trace[817219744] 'process raft request'  (duration: 118.27726ms)"],"step_count":1}
{"level":"info","ts":"2024-01-17T16:10:23.181669Z","caller":"traceutil/trace.go:171","msg":"trace[18260290] transaction","detail":"{read_only:false; response_revision:77309; number_of_response:1; }","duration":"110.644345ms","start":"2024-01-17T16:10:23.071005Z","end":"2024-01-17T16:10:23.181649Z","steps":["trace[18260290] 'process raft request'  (duration: 109.634191ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:23.47432Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.584253ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/prod/mysql-tp-0.17ab290b062ee544\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-17T16:10:23.47448Z","caller":"traceutil/trace.go:171","msg":"trace[2080031906] range","detail":"{range_begin:/registry/events/prod/mysql-tp-0.17ab290b062ee544; range_end:; response_count:0; response_revision:77311; }","duration":"104.253962ms","start":"2024-01-17T16:10:23.370185Z","end":"2024-01-17T16:10:23.474439Z","steps":["trace[2080031906] 'range keys from in-memory index tree'  (duration: 102.260482ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:23.474698Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.366016ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" ","response":"range_response_count:1 size:503"}
{"level":"info","ts":"2024-01-17T16:10:23.47477Z","caller":"traceutil/trace.go:171","msg":"trace[1646850581] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:77311; }","duration":"199.444972ms","start":"2024-01-17T16:10:23.275305Z","end":"2024-01-17T16:10:23.47475Z","steps":["trace[1646850581] 'range keys from in-memory index tree'  (duration: 199.177475ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:23.475722Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.364308ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/prod/data-mysql-tp-0\" ","response":"range_response_count:1 size:1405"}
{"level":"info","ts":"2024-01-17T16:10:23.475845Z","caller":"traceutil/trace.go:171","msg":"trace[1529437871] range","detail":"{range_begin:/registry/persistentvolumeclaims/prod/data-mysql-tp-0; range_end:; response_count:1; response_revision:77311; }","duration":"112.49082ms","start":"2024-01-17T16:10:23.363314Z","end":"2024-01-17T16:10:23.475804Z","steps":["trace[1529437871] 'range keys from in-memory index tree'  (duration: 111.932848ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-17T16:10:23.860145Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.093188ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/minikube-ingress-dns\" ","response":"range_response_count:1 size:907"}
{"level":"info","ts":"2024-01-17T16:10:23.860313Z","caller":"traceutil/trace.go:171","msg":"trace[961035757] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/minikube-ingress-dns; range_end:; response_count:1; response_revision:77313; }","duration":"100.264191ms","start":"2024-01-17T16:10:23.760007Z","end":"2024-01-17T16:10:23.860271Z","steps":["trace[961035757] 'agreement among raft nodes before linearized reading'  (duration: 99.972886ms)"],"step_count":1}
{"level":"info","ts":"2024-01-17T16:10:23.861037Z","caller":"traceutil/trace.go:171","msg":"trace[229278212] transaction","detail":"{read_only:false; response_revision:77313; number_of_response:1; }","duration":"190.560328ms","start":"2024-01-17T16:10:23.670442Z","end":"2024-01-17T16:10:23.861003Z","steps":["trace[229278212] 'process raft request'  (duration: 189.297099ms)"],"step_count":1}
{"level":"info","ts":"2024-01-17T16:10:23.977705Z","caller":"traceutil/trace.go:171","msg":"trace[1429883961] transaction","detail":"{read_only:false; response_revision:77314; number_of_response:1; }","duration":"104.528817ms","start":"2024-01-17T16:10:23.873126Z","end":"2024-01-17T16:10:23.977654Z","steps":["trace[1429883961] 'process raft request'  (duration: 103.536731ms)"],"step_count":1}
{"level":"info","ts":"2024-01-17T16:10:24.177888Z","caller":"traceutil/trace.go:171","msg":"trace[1350238531] transaction","detail":"{read_only:false; response_revision:77315; number_of_response:1; }","duration":"104.109331ms","start":"2024-01-17T16:10:24.073727Z","end":"2024-01-17T16:10:24.177836Z","steps":["trace[1350238531] 'process raft request'  (duration: 91.300636ms)","trace[1350238531] 'compare'  (duration: 12.590677ms)"],"step_count":2}

* 
* ==> etcd [e4d398a9621b] <==
* {"level":"info","ts":"2024-01-28T02:30:56.298858Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":324884}
{"level":"info","ts":"2024-01-28T02:30:56.30045Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":324884,"took":"1.376836ms","hash":2030483982}
{"level":"info","ts":"2024-01-28T02:30:56.300515Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2030483982,"revision":324884,"compact-revision":324603}
{"level":"info","ts":"2024-01-28T02:32:22.651874Z","caller":"traceutil/trace.go:171","msg":"trace[161459582] linearizableReadLoop","detail":"{readStateIndex:396958; appliedIndex:396957; }","duration":"112.016364ms","start":"2024-01-28T02:32:22.539831Z","end":"2024-01-28T02:32:22.651848Z","steps":["trace[161459582] 'read index received'  (duration: 92.159716ms)","trace[161459582] 'applied index is now lower than readState.Index'  (duration: 19.855666ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T02:32:22.652092Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.263042ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-28T02:32:22.652127Z","caller":"traceutil/trace.go:171","msg":"trace[944324732] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:325245; }","duration":"112.317066ms","start":"2024-01-28T02:32:22.5398Z","end":"2024-01-28T02:32:22.652117Z","steps":["trace[944324732] 'agreement among raft nodes before linearized reading'  (duration: 112.189649ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:32:22.652435Z","caller":"traceutil/trace.go:171","msg":"trace[1136859635] transaction","detail":"{read_only:false; response_revision:325245; number_of_response:1; }","duration":"115.44843ms","start":"2024-01-28T02:32:22.53696Z","end":"2024-01-28T02:32:22.652409Z","steps":["trace[1136859635] 'process raft request'  (duration: 95.106301ms)","trace[1136859635] 'compare'  (duration: 19.43008ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T02:35:02.450971Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.208086ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" ","response":"range_response_count:1 size:504"}
{"level":"info","ts":"2024-01-28T02:35:02.451145Z","caller":"traceutil/trace.go:171","msg":"trace[1088273219] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:325393; }","duration":"111.407955ms","start":"2024-01-28T02:35:02.339709Z","end":"2024-01-28T02:35:02.451117Z","steps":["trace[1088273219] 'range keys from in-memory index tree'  (duration: 110.949626ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T02:35:02.451665Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.244692ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-28T02:35:02.451743Z","caller":"traceutil/trace.go:171","msg":"trace[45360468] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:325393; }","duration":"117.34885ms","start":"2024-01-28T02:35:02.334373Z","end":"2024-01-28T02:35:02.451722Z","steps":["trace[45360468] 'range keys from in-memory index tree'  (duration: 117.033743ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:35:56.369016Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":325164}
{"level":"info","ts":"2024-01-28T02:35:56.370532Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":325164,"took":"1.196128ms","hash":434034294}
{"level":"info","ts":"2024-01-28T02:35:56.37063Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":434034294,"revision":325164,"compact-revision":324884}
{"level":"info","ts":"2024-01-28T02:36:17.435215Z","caller":"traceutil/trace.go:171","msg":"trace[215712610] transaction","detail":"{read_only:false; response_revision:325464; number_of_response:1; }","duration":"101.220372ms","start":"2024-01-28T02:36:17.333955Z","end":"2024-01-28T02:36:17.435175Z","steps":["trace[215712610] 'process raft request'  (duration: 101.000558ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T02:36:17.660973Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.117616ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-28T02:36:17.661201Z","caller":"traceutil/trace.go:171","msg":"trace[1486061650] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:325464; }","duration":"124.36952ms","start":"2024-01-28T02:36:17.536811Z","end":"2024-01-28T02:36:17.66118Z","steps":["trace[1486061650] 'range keys from in-memory index tree'  (duration: 123.874347ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:36:39.151796Z","caller":"traceutil/trace.go:171","msg":"trace[1469176688] transaction","detail":"{read_only:false; response_revision:325484; number_of_response:1; }","duration":"116.879513ms","start":"2024-01-28T02:36:39.034882Z","end":"2024-01-28T02:36:39.151762Z","steps":["trace[1469176688] 'process raft request'  (duration: 116.696336ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:36:50.131609Z","caller":"traceutil/trace.go:171","msg":"trace[1494425063] transaction","detail":"{read_only:false; response_revision:325495; number_of_response:1; }","duration":"191.038572ms","start":"2024-01-28T02:36:49.940543Z","end":"2024-01-28T02:36:50.131581Z","steps":["trace[1494425063] 'process raft request'  (duration: 190.692928ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:40:02.829653Z","caller":"traceutil/trace.go:171","msg":"trace[1448566035] transaction","detail":"{read_only:false; response_revision:325674; number_of_response:1; }","duration":"184.793687ms","start":"2024-01-28T02:40:02.644841Z","end":"2024-01-28T02:40:02.829635Z","steps":["trace[1448566035] 'process raft request'  (duration: 98.539171ms)","trace[1448566035] 'compare'  (duration: 85.846555ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T02:40:13.030243Z","caller":"traceutil/trace.go:171","msg":"trace[254418966] transaction","detail":"{read_only:false; response_revision:325685; number_of_response:1; }","duration":"115.2771ms","start":"2024-01-28T02:40:12.914907Z","end":"2024-01-28T02:40:13.030184Z","steps":["trace[254418966] 'process raft request'  (duration: 114.811229ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:40:56.38721Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":325445}
{"level":"info","ts":"2024-01-28T02:40:56.388289Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":325445,"took":"845.297¬µs","hash":3244798432}
{"level":"info","ts":"2024-01-28T02:40:56.388357Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3244798432,"revision":325445,"compact-revision":325164}
{"level":"info","ts":"2024-01-28T02:44:16.045369Z","caller":"traceutil/trace.go:171","msg":"trace[2441767] transaction","detail":"{read_only:false; response_revision:325913; number_of_response:1; }","duration":"110.727269ms","start":"2024-01-28T02:44:15.93462Z","end":"2024-01-28T02:44:16.045347Z","steps":["trace[2441767] 'process raft request'  (duration: 110.58429ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:45:56.410929Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":325725}
{"level":"info","ts":"2024-01-28T02:45:56.412008Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":325725,"took":"857.987¬µs","hash":699435884}
{"level":"info","ts":"2024-01-28T02:45:56.412101Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":699435884,"revision":325725,"compact-revision":325445}
{"level":"info","ts":"2024-01-28T02:46:22.345526Z","caller":"traceutil/trace.go:171","msg":"trace[1638769343] linearizableReadLoop","detail":"{readStateIndex:397919; appliedIndex:397918; }","duration":"109.273674ms","start":"2024-01-28T02:46:22.236225Z","end":"2024-01-28T02:46:22.345499Z","steps":["trace[1638769343] 'read index received'  (duration: 109.109346ms)","trace[1638769343] 'applied index is now lower than readState.Index'  (duration: 163.357¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-28T02:46:22.345653Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.502474ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-28T02:46:22.345689Z","caller":"traceutil/trace.go:171","msg":"trace[1617521398] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:326030; }","duration":"109.55897ms","start":"2024-01-28T02:46:22.23612Z","end":"2024-01-28T02:46:22.345679Z","steps":["trace[1617521398] 'agreement among raft nodes before linearized reading'  (duration: 109.476976ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:46:22.345689Z","caller":"traceutil/trace.go:171","msg":"trace[1834342444] transaction","detail":"{read_only:false; response_revision:326030; number_of_response:1; }","duration":"114.432622ms","start":"2024-01-28T02:46:22.231221Z","end":"2024-01-28T02:46:22.345654Z","steps":["trace[1834342444] 'process raft request'  (duration: 114.141665ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:46:54.041344Z","caller":"traceutil/trace.go:171","msg":"trace[913169986] transaction","detail":"{read_only:false; response_revision:326060; number_of_response:1; }","duration":"102.107975ms","start":"2024-01-28T02:46:53.939204Z","end":"2024-01-28T02:46:54.041312Z","steps":["trace[913169986] 'process raft request'  (duration: 101.909783ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:50:34.552832Z","caller":"traceutil/trace.go:171","msg":"trace[1077289705] transaction","detail":"{read_only:false; response_revision:326266; number_of_response:1; }","duration":"115.641938ms","start":"2024-01-28T02:50:34.437167Z","end":"2024-01-28T02:50:34.552809Z","steps":["trace[1077289705] 'process raft request'  (duration: 115.245051ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:50:56.435636Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326005}
{"level":"info","ts":"2024-01-28T02:50:56.436896Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326005,"took":"1.032402ms","hash":3987952142}
{"level":"info","ts":"2024-01-28T02:50:56.436979Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3987952142,"revision":326005,"compact-revision":325725}
{"level":"info","ts":"2024-01-28T02:55:07.825484Z","caller":"traceutil/trace.go:171","msg":"trace[2026080806] transaction","detail":"{read_only:false; response_revision:326523; number_of_response:1; }","duration":"191.697891ms","start":"2024-01-28T02:55:07.63372Z","end":"2024-01-28T02:55:07.825418Z","steps":["trace[2026080806] 'process raft request'  (duration: 191.548269ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T02:55:56.454519Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326286}
{"level":"info","ts":"2024-01-28T02:55:56.455662Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326286,"took":"829.963¬µs","hash":2758833658}
{"level":"info","ts":"2024-01-28T02:55:56.455749Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2758833658,"revision":326286,"compact-revision":326005}
{"level":"info","ts":"2024-01-28T03:00:56.47728Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326569}
{"level":"info","ts":"2024-01-28T03:00:56.478368Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326569,"took":"883.142¬µs","hash":2788682768}
{"level":"info","ts":"2024-01-28T03:00:56.478455Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2788682768,"revision":326569,"compact-revision":326286}
{"level":"info","ts":"2024-01-28T03:01:26.248742Z","caller":"traceutil/trace.go:171","msg":"trace[1869089731] transaction","detail":"{read_only:false; response_revision:326879; number_of_response:1; }","duration":"103.285131ms","start":"2024-01-28T03:01:26.145433Z","end":"2024-01-28T03:01:26.248718Z","steps":["trace[1869089731] 'process raft request'  (duration: 103.158027ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T03:01:36.850187Z","caller":"traceutil/trace.go:171","msg":"trace[1889324612] transaction","detail":"{read_only:false; response_revision:326889; number_of_response:1; }","duration":"113.606287ms","start":"2024-01-28T03:01:36.736557Z","end":"2024-01-28T03:01:36.850164Z","steps":["trace[1889324612] 'process raft request'  (duration: 113.389324ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T03:03:53.635254Z","caller":"traceutil/trace.go:171","msg":"trace[863726150] transaction","detail":"{read_only:false; response_revision:327019; number_of_response:1; }","duration":"197.857808ms","start":"2024-01-28T03:03:53.437372Z","end":"2024-01-28T03:03:53.63523Z","steps":["trace[863726150] 'process raft request'  (duration: 197.311709ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T03:04:04.141812Z","caller":"traceutil/trace.go:171","msg":"trace[522756342] transaction","detail":"{read_only:false; response_revision:327029; number_of_response:1; }","duration":"104.704433ms","start":"2024-01-28T03:04:04.037086Z","end":"2024-01-28T03:04:04.141791Z","steps":["trace[522756342] 'process raft request'  (duration: 104.541706ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T03:05:56.496715Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326852}
{"level":"info","ts":"2024-01-28T03:05:56.497718Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326852,"took":"753.855¬µs","hash":3040605808}
{"level":"info","ts":"2024-01-28T03:05:56.497797Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3040605808,"revision":326852,"compact-revision":326569}
{"level":"info","ts":"2024-01-28T03:07:02.443177Z","caller":"traceutil/trace.go:171","msg":"trace[1779006432] transaction","detail":"{read_only:false; response_revision:327193; number_of_response:1; }","duration":"101.525223ms","start":"2024-01-28T03:07:02.341589Z","end":"2024-01-28T03:07:02.443114Z","steps":["trace[1779006432] 'process raft request'  (duration: 101.007689ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-28T03:07:13.156352Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.768306ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026721009472711 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:327194 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128026721009472709 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-01-28T03:07:13.1565Z","caller":"traceutil/trace.go:171","msg":"trace[930911359] transaction","detail":"{read_only:false; response_revision:327203; number_of_response:1; }","duration":"208.464178ms","start":"2024-01-28T03:07:12.948016Z","end":"2024-01-28T03:07:13.15648Z","steps":["trace[930911359] 'process raft request'  (duration: 83.72466ms)","trace[930911359] 'compare'  (duration: 123.492375ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T03:07:55.141911Z","caller":"traceutil/trace.go:171","msg":"trace[1059417510] transaction","detail":"{read_only:false; response_revision:327242; number_of_response:1; }","duration":"101.731273ms","start":"2024-01-28T03:07:55.040123Z","end":"2024-01-28T03:07:55.141855Z","steps":["trace[1059417510] 'process raft request'  (duration: 101.373088ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T03:10:33.137245Z","caller":"traceutil/trace.go:171","msg":"trace[282959112] transaction","detail":"{read_only:false; response_revision:327391; number_of_response:1; }","duration":"186.824703ms","start":"2024-01-28T03:10:32.95039Z","end":"2024-01-28T03:10:33.137214Z","steps":["trace[282959112] 'process raft request'  (duration: 99.987653ms)","trace[282959112] 'compare'  (duration: 86.357188ms)"],"step_count":2}
{"level":"info","ts":"2024-01-28T03:10:33.137263Z","caller":"traceutil/trace.go:171","msg":"trace[1101187153] transaction","detail":"{read_only:false; response_revision:327392; number_of_response:1; }","duration":"101.570539ms","start":"2024-01-28T03:10:33.035673Z","end":"2024-01-28T03:10:33.137243Z","steps":["trace[1101187153] 'process raft request'  (duration: 101.232196ms)"],"step_count":1}
{"level":"info","ts":"2024-01-28T03:10:56.522634Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":327133}
{"level":"info","ts":"2024-01-28T03:10:56.523835Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":327133,"took":"837.616¬µs","hash":1501113075}
{"level":"info","ts":"2024-01-28T03:10:56.52391Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1501113075,"revision":327133,"compact-revision":326852}

* 
* ==> kernel <==
*  03:13:16 up 3 days,  7:45,  0 users,  load average: 0.35, 0.41, 0.50
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [41c74e564dff] <==
* I0117 15:22:00.675407       1 trace.go:236] Trace[1854505130]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (17-Jan-2024 15:22:00.083) (total time: 592ms):
Trace[1854505130]: ---"initial value restored" 92ms (15:22:00.176)
Trace[1854505130]: ---"Transaction prepared" 198ms (15:22:00.374)
Trace[1854505130]: ---"Txn call completed" 300ms (15:22:00.675)
Trace[1854505130]: [592.026105ms] [592.026105ms] END
I0117 16:02:51.119126       1 trace.go:236] Trace[224213389]: "Update" accept:application/json, */*,audit-id:8d55caaa-5127-403e-a468-c29563d477e0,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (17-Jan-2024 16:02:50.427) (total time: 691ms):
Trace[224213389]: ["GuaranteedUpdate etcd3" audit-id:8d55caaa-5127-403e-a468-c29563d477e0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 690ms (16:02:50.428)
Trace[224213389]:  ---"Txn call completed" 690ms (16:02:51.118)]
Trace[224213389]: [691.182096ms] [691.182096ms] END
I0117 16:09:55.761384       1 trace.go:236] Trace[1480472811]: "Update" accept:application/json, */*,audit-id:7c3ea397-7e37-456f-85a1-69e338e0ca73,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (17-Jan-2024 16:09:55.062) (total time: 698ms):
Trace[1480472811]: ["GuaranteedUpdate etcd3" audit-id:7c3ea397-7e37-456f-85a1-69e338e0ca73,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 696ms (16:09:55.064)
Trace[1480472811]:  ---"About to Encode" 203ms (16:09:55.268)
Trace[1480472811]:  ---"Txn call completed" 490ms (16:09:55.759)]
Trace[1480472811]: [698.225234ms] [698.225234ms] END
I0117 16:09:56.472574       1 trace.go:236] Trace[491420293]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (17-Jan-2024 16:09:55.362) (total time: 1110ms):
Trace[491420293]: ---"initial value restored" 316ms (16:09:55.678)
Trace[491420293]: ---"Transaction prepared" 502ms (16:09:56.180)
Trace[491420293]: ---"Txn call completed" 291ms (16:09:56.471)
Trace[491420293]: [1.110229875s] [1.110229875s] END
I0117 16:09:56.800432       1 trace.go:236] Trace[66705839]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e2321166-abcf-4cdf-a1b0-fd225a4e93d6,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (17-Jan-2024 16:09:56.176) (total time: 623ms):
Trace[66705839]: ---"limitedReadBody succeeded" len:476 84ms (16:09:56.261)
Trace[66705839]: ["GuaranteedUpdate etcd3" audit-id:e2321166-abcf-4cdf-a1b0-fd225a4e93d6,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 538ms (16:09:56.262)
Trace[66705839]:  ---"About to Encode" 99ms (16:09:56.362)
Trace[66705839]:  ---"Txn call completed" 437ms (16:09:56.799)]
Trace[66705839]: [623.458814ms] [623.458814ms] END
E0117 16:10:22.459295       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0117 16:10:22.459605       1 status.go:71] apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:"etcdserver: request timed out"}: etcdserver: request timed out
E0117 16:10:22.459631       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0117 16:10:22.464652       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0117 16:10:22.464747       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0117 16:10:22.465949       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0117 16:10:22.466047       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0117 16:10:22.466107       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I0117 16:10:22.467332       1 trace.go:236] Trace[388788322]: "Update" accept:application/json, */*,audit-id:b8c65bdc-c940-4076-9738-5766c75930a0,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (17-Jan-2024 16:09:58.162) (total time: 24303ms):
Trace[388788322]: ["GuaranteedUpdate etcd3" audit-id:b8c65bdc-c940-4076-9738-5766c75930a0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 24302ms (16:09:58.163)
Trace[388788322]:  ---"Txn call failed" err:etcdserver: request timed out 24104ms (16:10:22.271)]
Trace[388788322]: [24.303396929s] [24.303396929s] END
I0117 16:10:22.467863       1 trace.go:236] Trace[257701427]: "Get" accept:application/json, */*,audit-id:54a711d2-f12f-41f7-82fd-326002e6783e,client:10.244.0.15,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.9.4 (linux/amd64) ingress-nginx/846d251814a09d8a5d8d28e2e604bfc7749bcb49,verb:GET (17-Jan-2024 16:10:02.062) (total time: 20404ms):
Trace[257701427]: [20.404149267s] [20.404149267s] END
E0117 16:10:22.559471       1 timeout.go:142] post-timeout activity - time-elapsed: 102.585181ms, PUT "/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result: <nil>
E0117 16:10:22.559837       1 timeout.go:142] post-timeout activity - time-elapsed: 103.428357ms, GET "/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader" result: <nil>
I0117 16:10:22.869076       1 trace.go:236] Trace[2073368857]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d7e08e29-eba5-4952-a661-1c6b198c5e49,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (17-Jan-2024 16:10:22.277) (total time: 514ms):
Trace[2073368857]: ["GuaranteedUpdate etcd3" audit-id:d7e08e29-eba5-4952-a661-1c6b198c5e49,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 514ms (16:10:22.278)
Trace[2073368857]:  ---"Txn call completed" 512ms (16:10:22.792)]
Trace[2073368857]: [514.850117ms] [514.850117ms] END
I0117 16:10:23.075367       1 trace.go:236] Trace[1809805856]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:aa46c0e5-f122-4cb7-a585-c6aa6561b84d,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (17-Jan-2024 16:10:22.270) (total time: 805ms):
Trace[1809805856]: ["Create etcd3" audit-id:aa46c0e5-f122-4cb7-a585-c6aa6561b84d,key:/events/kube-system/kube-apiserver-minikube.17ab2e4f5b482de4,type:*core.Event,resource:events 714ms (16:10:22.360)
Trace[1809805856]:  ---"TransformToStorage succeeded" 421ms (16:10:22.782)
Trace[1809805856]:  ---"Txn call succeeded" 292ms (16:10:23.074)]
Trace[1809805856]: [805.100112ms] [805.100112ms] END
E0117 16:10:23.174622       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0117 16:10:23.174674       1 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 47.766¬µs, panicked: false, err: context canceled, panic-reason: <nil>
E0117 16:10:23.174687       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0117 16:10:23.177154       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0117 16:10:23.177542       1 timeout.go:142] post-timeout activity - time-elapsed: 9.951087ms, POST "/api/v1/namespaces/kube-system/events" result: <nil>
I0117 16:10:23.260220       1 trace.go:236] Trace[1106983222]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (17-Jan-2024 16:10:22.273) (total time: 987ms):
Trace[1106983222]: ---"initial value restored" 595ms (16:10:22.868)
Trace[1106983222]: ---"Transaction prepared" 192ms (16:10:23.060)
Trace[1106983222]: ---"Txn call completed" 199ms (16:10:23.260)
Trace[1106983222]: [987.023829ms] [987.023829ms] END

* 
* ==> kube-apiserver [919612816b57] <==
* Trace[1701880247]:  ---"Txn call completed" 772ms (03:17:42.749)]
Trace[1701880247]: [774.810165ms] [774.810165ms] END
I0127 03:29:37.592377       1 trace.go:236] Trace[1818145256]: "Update" accept:application/json, */*,audit-id:d51a7a99-3027-47be-8ca1-85c46743c897,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (27-Jan-2024 03:29:36.973) (total time: 619ms):
Trace[1818145256]: ["GuaranteedUpdate etcd3" audit-id:d51a7a99-3027-47be-8ca1-85c46743c897,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 619ms (03:29:36.973)
Trace[1818145256]:  ---"Txn call completed" 618ms (03:29:37.592)]
Trace[1818145256]: [619.194905ms] [619.194905ms] END
I0127 03:29:37.784278       1 trace.go:236] Trace[1628179148]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (27-Jan-2024 03:29:36.831) (total time: 953ms):
Trace[1628179148]: ---"initial value restored" 140ms (03:29:36.971)
Trace[1628179148]: ---"Transaction prepared" 620ms (03:29:37.592)
Trace[1628179148]: ---"Txn call completed" 192ms (03:29:37.784)
Trace[1628179148]: [953.019965ms] [953.019965ms] END
I0127 11:16:02.626238       1 trace.go:236] Trace[817293933]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (27-Jan-2024 11:16:02.112) (total time: 513ms):
Trace[817293933]: ---"Transaction prepared" 160ms (11:16:02.301)
Trace[817293933]: ---"Txn call completed" 324ms (11:16:02.626)
Trace[817293933]: [513.346992ms] [513.346992ms] END
I0127 11:17:34.029151       1 trace.go:236] Trace[640313848]: "Update" accept:application/json, */*,audit-id:d5c79b2f-3240-4a27-bc29-ede0d30ed4ca,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (27-Jan-2024 11:17:33.269) (total time: 759ms):
Trace[640313848]: ["GuaranteedUpdate etcd3" audit-id:d5c79b2f-3240-4a27-bc29-ede0d30ed4ca,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 759ms (11:17:33.269)
Trace[640313848]:  ---"Txn call completed" 758ms (11:17:34.028)]
Trace[640313848]: [759.51448ms] [759.51448ms] END
I0127 11:17:36.676917       1 trace.go:236] Trace[1140094770]: "Get" accept:application/json, */*,audit-id:202ad4a5-dcc4-4410-88d6-3866a2942eb1,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (27-Jan-2024 11:17:36.032) (total time: 644ms):
Trace[1140094770]: ---"About to write a response" 644ms (11:17:36.676)
Trace[1140094770]: [644.25002ms] [644.25002ms] END
I0128 00:17:34.112920       1 trace.go:236] Trace[2040761337]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b2a3b113-5999-4a9a-b1b3-75cb74f33bf1,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (28-Jan-2024 00:17:33.604) (total time: 508ms):
Trace[2040761337]: ["GuaranteedUpdate etcd3" audit-id:b2a3b113-5999-4a9a-b1b3-75cb74f33bf1,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 508ms (00:17:33.604)
Trace[2040761337]:  ---"Txn call completed" 507ms (00:17:34.112)]
Trace[2040761337]: [508.360246ms] [508.360246ms] END
I0128 00:17:33.671092       1 trace.go:236] Trace[1224551269]: "Update" accept:application/json, */*,audit-id:3fc108bb-d469-4c91-bb67-fc58db2e30a2,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 00:17:32.556) (total time: 658ms):
Trace[1224551269]: ["GuaranteedUpdate etcd3" audit-id:3fc108bb-d469-4c91-bb67-fc58db2e30a2,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 658ms (00:17:32.556)
Trace[1224551269]:  ---"Txn call completed" 657ms (00:17:33.214)]
Trace[1224551269]: [658.661924ms] [658.661924ms] END
I0128 00:17:37.521003       1 trace.go:236] Trace[638371475]: "Get" accept:application/json, */*,audit-id:174e268c-ca15-4877-9f53-f03e6c92e7ca,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (28-Jan-2024 00:17:36.473) (total time: 1047ms):
Trace[638371475]: ---"About to write a response" 1047ms (00:17:37.520)
Trace[638371475]: [1.047508016s] [1.047508016s] END
I0128 00:17:37.521237       1 trace.go:236] Trace[1196909193]: "Get" accept:application/json, */*,audit-id:bd6a1e81-d91c-4850-b427-c34ac5c2f414,client:10.244.0.28,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.9.4 (linux/amd64) ingress-nginx/846d251814a09d8a5d8d28e2e604bfc7749bcb49,verb:GET (28-Jan-2024 00:17:36.763) (total time: 758ms):
Trace[1196909193]: ---"About to write a response" 758ms (00:17:37.521)
Trace[1196909193]: [758.183699ms] [758.183699ms] END
I0128 00:17:37.521510       1 trace.go:236] Trace[47017060]: "List" accept:application/json, */*,audit-id:0c3df6ee-67ce-4a95-93bb-5ccd9ddb791c,client:10.244.0.28,protocol:HTTP/2.0,resource:pods,scope:namespace,url:/api/v1/namespaces/ingress-nginx/pods,user-agent:nginx-ingress-controller/v1.9.4 (linux/amd64) ingress-nginx/846d251814a09d8a5d8d28e2e604bfc7749bcb49,verb:LIST (28-Jan-2024 00:17:36.990) (total time: 530ms):
Trace[47017060]: ["List(recursive=true) etcd3" audit-id:0c3df6ee-67ce-4a95-93bb-5ccd9ddb791c,key:/pods/ingress-nginx,resourceVersion:,resourceVersionMatch:,limit:0,continue: 530ms (00:17:36.990)]
Trace[47017060]: [530.993387ms] [530.993387ms] END
I0128 00:17:38.075424       1 trace.go:236] Trace[1986687458]: "Update" accept:application/json, */*,audit-id:4efb6a85-88e3-4f05-99b6-ca18487c393b,client:10.244.0.28,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.9.4 (linux/amd64) ingress-nginx/846d251814a09d8a5d8d28e2e604bfc7749bcb49,verb:PUT (28-Jan-2024 00:17:37.522) (total time: 552ms):
Trace[1986687458]: ["GuaranteedUpdate etcd3" audit-id:4efb6a85-88e3-4f05-99b6-ca18487c393b,key:/leases/ingress-nginx/ingress-nginx-leader,type:*coordination.Lease,resource:leases.coordination.k8s.io 552ms (00:17:37.522)
Trace[1986687458]:  ---"Txn call completed" 551ms (00:17:38.075)]
Trace[1986687458]: [552.606576ms] [552.606576ms] END
I0128 00:17:38.075712       1 trace.go:236] Trace[476079602]: "Update" accept:application/json, */*,audit-id:451633e6-2c2b-4404-a591-9cc49e821d2a,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (28-Jan-2024 00:17:37.522) (total time: 552ms):
Trace[476079602]: ["GuaranteedUpdate etcd3" audit-id:451633e6-2c2b-4404-a591-9cc49e821d2a,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 552ms (00:17:37.522)
Trace[476079602]:  ---"Txn call completed" 551ms (00:17:38.075)]
Trace[476079602]: [552.884931ms] [552.884931ms] END
I0128 01:00:28.405800       1 trace.go:236] Trace[1777704986]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0ebf7b99-340b-47dd-bb5d-4377425915f9,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (28-Jan-2024 01:00:27.774) (total time: 630ms):
Trace[1777704986]: ["GuaranteedUpdate etcd3" audit-id:0ebf7b99-340b-47dd-bb5d-4377425915f9,key:/minions/minikube,type:*core.Node,resource:nodes 630ms (01:00:27.774)
Trace[1777704986]:  ---"Txn call completed" 628ms (01:00:28.405)]
Trace[1777704986]: ---"Object stored in database" 628ms (01:00:28.405)
Trace[1777704986]: [630.96631ms] [630.96631ms] END
I0128 02:11:32.759041       1 trace.go:236] Trace[951448075]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (28-Jan-2024 02:11:32.145) (total time: 612ms):
Trace[951448075]: ---"initial value restored" 171ms (02:11:32.317)
Trace[951448075]: ---"Transaction prepared" 436ms (02:11:32.753)
Trace[951448075]: [612.624499ms] [612.624499ms] END
I0128 02:11:35.324066       1 trace.go:236] Trace[1214117137]: "Update" accept:application/json, */*,audit-id:777f35a2-82ba-4e93-a330-0e13a1017a05,client:10.244.0.28,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.9.4 (linux/amd64) ingress-nginx/846d251814a09d8a5d8d28e2e604bfc7749bcb49,verb:PUT (28-Jan-2024 02:11:34.449) (total time: 874ms):
Trace[1214117137]: ["GuaranteedUpdate etcd3" audit-id:777f35a2-82ba-4e93-a330-0e13a1017a05,key:/leases/ingress-nginx/ingress-nginx-leader,type:*coordination.Lease,resource:leases.coordination.k8s.io 874ms (02:11:34.449)
Trace[1214117137]:  ---"Txn call completed" 873ms (02:11:35.323)]
Trace[1214117137]: [874.444761ms] [874.444761ms] END

* 
* ==> kube-controller-manager [618a2c0ae050] <==
* I0117 11:33:23.530636       1 event.go:307] "Event occurred" object="default/backend-57cd8d587" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-57cd8d587-gw2f2"
I0117 11:33:23.542357       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="26.4781ms"
I0117 11:33:23.563988       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="21.481069ms"
I0117 11:33:23.564076       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="53.912¬µs"
I0117 11:33:25.693473       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="79.491¬µs"
I0117 11:33:40.592216       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="76.695¬µs"
I0117 11:33:53.591627       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="97.584¬µs"
I0117 11:34:07.589673       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="110.438¬µs"
I0117 11:34:23.589156       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="78.477¬µs"
I0117 11:34:35.589898       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="79.409¬µs"
I0117 11:35:15.595077       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="132.601¬µs"
I0117 11:35:26.589032       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="96.512¬µs"
I0117 11:36:44.591954       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="88.006¬µs"
I0117 11:36:59.589252       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="86.352¬µs"
I0117 11:39:28.617064       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="97.253¬µs"
I0117 11:39:41.588502       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="122.603¬µs"
I0117 11:44:35.600057       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="80.792¬µs"
I0117 11:44:49.598828       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="104.987¬µs"
I0117 11:49:36.354679       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="17.394061ms"
I0117 11:49:36.354816       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="67.166¬µs"
I0117 11:52:18.780712       1 event.go:307] "Event occurred" object="default/backend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set backend-57cd8d587 to 3 from 1"
I0117 11:52:18.794865       1 event.go:307] "Event occurred" object="default/backend-57cd8d587" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-57cd8d587-zxwqs"
I0117 11:52:18.802522       1 event.go:307] "Event occurred" object="default/backend-57cd8d587" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: backend-57cd8d587-ht455"
I0117 11:52:18.817122       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="36.61525ms"
I0117 11:52:18.827944       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="10.743467ms"
I0117 11:52:18.828163       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="42.419¬µs"
I0117 11:52:18.828292       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="33.092¬µs"
I0117 11:52:18.853662       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="85.761¬µs"
I0117 11:52:21.003079       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="7.804524ms"
I0117 11:52:21.003237       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="68.188¬µs"
I0117 11:52:22.014603       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="11.195019ms"
I0117 11:52:22.014736       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="65.685¬µs"
I0117 11:52:51.120019       1 event.go:307] "Event occurred" object="default/backend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set backend-57cd8d587 to 1 from 3"
I0117 11:52:51.132282       1 event.go:307] "Event occurred" object="default/backend-57cd8d587" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: backend-57cd8d587-ht455"
I0117 11:52:51.140257       1 event.go:307] "Event occurred" object="default/backend-57cd8d587" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: backend-57cd8d587-zxwqs"
I0117 11:52:51.150880       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="31.183928ms"
I0117 11:52:51.161570       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="10.587671ms"
I0117 11:52:51.161760       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="71.575¬µs"
I0117 11:52:51.489190       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="69.221¬µs"
I0117 11:52:51.578120       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="44.634¬µs"
I0117 11:52:51.779174       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="78.408¬µs"
I0117 11:52:51.791189       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="94.478¬µs"
I0117 11:52:51.794558       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="103.365¬µs"
I0117 11:52:51.807434       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="57.118¬µs"
I0117 11:52:51.864139       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="65.513¬µs"
I0117 11:52:51.870567       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="57.618¬µs"
I0117 12:00:19.101865       1 event.go:307] "Event occurred" object="default/frontend" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set frontend-7c849d7bfd to 1"
I0117 12:00:19.130207       1 event.go:307] "Event occurred" object="default/frontend-7c849d7bfd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-7c849d7bfd-n9c8n"
I0117 12:00:19.157339       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="55.725957ms"
I0117 12:00:19.166859       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="9.427504ms"
I0117 12:00:19.167048       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="77.355¬µs"
I0117 12:00:19.178010       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="93.596¬µs"
I0117 12:01:01.321991       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="7.720384ms"
I0117 12:01:01.322132       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="59.322¬µs"
I0117 12:17:00.081213       1 event.go:307] "Event occurred" object="prod/mysql-tp" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Claim data-mysql-tp-0 Pod mysql-tp-0 in StatefulSet mysql-tp success"
I0117 12:17:00.092236       1 event.go:307] "Event occurred" object="prod/mysql-tp" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod mysql-tp-0 in StatefulSet mysql-tp successful"
I0117 12:17:00.105076       1 event.go:307] "Event occurred" object="prod/data-mysql-tp-0" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
E0117 14:33:31.714891       1 node_lifecycle_controller.go:971] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" node="minikube"
I0117 14:33:31.811435       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0117 14:33:41.824279       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"

* 
* ==> kube-controller-manager [97481c61de5a] <==
* I0125 19:00:52.006807       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="303.7726ms"
I0125 19:00:52.012589       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68-7hzfs" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:00:52.108187       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="115.529¬µs"
I0125 19:00:52.898688       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="889.937561ms"
I0125 19:00:52.899056       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="200.66¬µs"
I0125 19:00:52.994845       1 event.go:307] "Event occurred" object="kube-system/kube-proxy-bkhn7" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:00:53.398458       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-x5pg2" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:00:53.404213       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0125 19:00:54.794538       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="1.483906749s"
I0125 19:00:54.794766       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="146.718¬µs"
I0125 19:00:58.406268       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0125 19:01:00.112029       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="112.021085ms"
I0125 19:01:00.113723       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="137.871¬µs"
I0125 19:01:00.299692       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-7586cdbcd4" duration="177.383941ms"
I0125 19:01:00.300674       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-7586cdbcd4" duration="161.976¬µs"
I0125 19:01:00.608846       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="32.312099ms"
I0125 19:01:00.611814       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="98.517¬µs"
I0125 19:01:00.693876       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="81.422392ms"
I0125 19:01:00.697302       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="104.308¬µs"
I0125 19:01:00.797972       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="85.855095ms"
I0125 19:01:00.798271       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="122.702¬µs"
I0125 19:01:00.930994       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/myservice2-bb48c65" duration="14.925971ms"
I0125 19:01:00.931212       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/myservice2-bb48c65" duration="72.988¬µs"
I0125 19:01:01.005495       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="63.657757ms"
I0125 19:01:01.005728       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="97.615¬µs"
I0125 19:01:01.110338       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="87.760341ms"
I0125 19:01:01.113466       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="83.017¬µs"
I0126 10:44:21.010351       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I0126 10:44:21.012794       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I0126 16:08:24.174661       1 event.go:307] "Event occurred" object="default/api" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set api-5ffb9d968f to 1"
I0126 16:08:24.217638       1 event.go:307] "Event occurred" object="default/api-5ffb9d968f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: api-5ffb9d968f-65972"
I0126 16:08:24.239026       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="74.442174ms"
I0126 16:08:24.252908       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="13.76914ms"
I0126 16:08:24.274928       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="206.259¬µs"
I0126 16:08:24.316087       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="155.994¬µs"
I0126 16:08:24.372062       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="560.987¬µs"
I0126 16:08:38.041125       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="802.277¬µs"
I0126 16:23:44.968081       1 event.go:307] "Event occurred" object="default/site" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set site-55857bc65 to 1"
I0126 16:23:44.993814       1 event.go:307] "Event occurred" object="default/site-55857bc65" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: site-55857bc65-tcsfg"
I0126 16:23:45.009993       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="41.530747ms"
I0126 16:23:45.032753       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="22.682423ms"
I0126 16:23:45.032936       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="99.578¬µs"
I0126 16:23:45.086913       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="107.743¬µs"
I0126 16:26:18.071793       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="77.448¬µs"
I0126 16:39:10.782374       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="128.602¬µs"
I0126 16:39:24.776701       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="82.595¬µs"
I0127 11:10:21.911748       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="1.860984ms"
I0127 11:10:21.911693       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/backend-57cd8d587" duration="1.920156ms"
I0127 11:10:21.911685       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="2.224259ms"
I0127 11:10:21.911685       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-5ffb9d968f" duration="2.202328ms"
I0127 11:10:21.912618       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/api-7586cdbcd4" duration="58.289¬µs"
I0127 11:10:21.912609       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="67.136¬µs"
I0127 11:10:21.912620       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-7c849d7bfd" duration="49.854¬µs"
I0127 11:10:21.911687       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="2.120233ms"
I0127 11:10:21.912620       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="521.072¬µs"
I0127 11:10:21.913249       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/myservice2-bb48c65" duration="582.197¬µs"
I0127 18:11:39.690749       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-create"
I0127 18:11:39.691137       1 job_controller.go:562] "enqueueing job" key="ingress-nginx/ingress-nginx-admission-patch"
I0127 19:52:29.345324       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="125.827¬µs"
I0127 19:52:42.310409       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/site-55857bc65" duration="114.992¬µs"

* 
* ==> kube-proxy [044a778148bc] <==
* I0117 11:04:39.594357       1 server_others.go:69] "Using iptables proxy"
I0117 11:04:39.887182       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0117 11:04:40.328791       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0117 11:04:40.335725       1 server_others.go:152] "Using iptables Proxier"
I0117 11:04:40.335839       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0117 11:04:40.335853       1 server_others.go:438] "Defaulting to no-op detect-local"
I0117 11:04:40.335920       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0117 11:04:40.343046       1 server.go:846] "Version info" version="v1.28.3"
I0117 11:04:40.343110       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0117 11:04:40.360624       1 config.go:188] "Starting service config controller"
I0117 11:04:40.364115       1 config.go:97] "Starting endpoint slice config controller"
I0117 11:04:40.370511       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0117 11:04:40.370522       1 shared_informer.go:311] Waiting for caches to sync for service config
I0117 11:04:40.371458       1 config.go:315] "Starting node config controller"
I0117 11:04:40.372977       1 shared_informer.go:311] Waiting for caches to sync for node config
I0117 11:04:40.471975       1 shared_informer.go:318] Caches are synced for service config
I0117 11:04:40.472031       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0117 11:04:40.473884       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [86cb1dc6236e] <==
* I0124 20:08:58.076553       1 server_others.go:69] "Using iptables proxy"
I0124 20:08:58.890641       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0124 20:09:01.293814       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0124 20:09:01.305228       1 server_others.go:152] "Using iptables Proxier"
I0124 20:09:01.369755       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0124 20:09:01.369840       1 server_others.go:438] "Defaulting to no-op detect-local"
I0124 20:09:01.371168       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0124 20:09:01.381066       1 server.go:846] "Version info" version="v1.28.3"
I0124 20:09:01.381148       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0124 20:09:01.482816       1 config.go:188] "Starting service config controller"
I0124 20:09:01.483579       1 shared_informer.go:311] Waiting for caches to sync for service config
I0124 20:09:01.483832       1 config.go:315] "Starting node config controller"
I0124 20:09:01.483847       1 shared_informer.go:311] Waiting for caches to sync for node config
I0124 20:09:01.483884       1 config.go:97] "Starting endpoint slice config controller"
I0124 20:09:01.483898       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0124 20:09:01.584624       1 shared_informer.go:318] Caches are synced for node config
I0124 20:09:01.584741       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0124 20:09:01.784045       1 shared_informer.go:318] Caches are synced for service config
I0125 08:42:04.279939       1 trace.go:236] Trace[1272713768]: "iptables save" (25-Jan-2024 08:42:01.969) (total time: 2205ms):
Trace[1272713768]: [2.205051844s] [2.205051844s] END
I0125 12:08:25.854565       1 trace.go:236] Trace[1600795761]: "iptables ChainExists" (25-Jan-2024 12:08:20.646) (total time: 4400ms):
Trace[1600795761]: [4.400929612s] [4.400929612s] END
I0125 12:55:52.273750       1 trace.go:236] Trace[1868596551]: "iptables save" (25-Jan-2024 12:55:46.769) (total time: 4659ms):
Trace[1868596551]: [4.659804609s] [4.659804609s] END
I0125 12:55:52.877116       1 trace.go:236] Trace[1475453156]: "iptables ChainExists" (25-Jan-2024 12:55:46.699) (total time: 4626ms):
Trace[1475453156]: [4.626687042s] [4.626687042s] END
I0125 19:00:41.701962       1 trace.go:236] Trace[1662066577]: "iptables save" (25-Jan-2024 19:00:05.342) (total time: 34206ms):
Trace[1662066577]: [34.206902376s] [34.206902376s] END
I0125 19:00:41.897064       1 trace.go:236] Trace[1964558929]: "iptables save" (25-Jan-2024 19:00:05.042) (total time: 34607ms):
Trace[1964558929]: [34.607857263s] [34.607857263s] END
I0126 15:49:09.854121       1 trace.go:236] Trace[1005376860]: "iptables ChainExists" (26-Jan-2024 15:49:01.182) (total time: 7498ms):
Trace[1005376860]: [7.498516414s] [7.498516414s] END
I0126 15:49:09.854022       1 trace.go:236] Trace[1798003696]: "iptables ChainExists" (26-Jan-2024 15:49:01.170) (total time: 7618ms):
Trace[1798003696]: [7.61800889s] [7.61800889s] END

* 
* ==> kube-scheduler [029e6818a6d5] <==
* Trace[352108263]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10038ms (20:08:31.777)
Trace[352108263]: [10.038948108s] [10.038948108s] END
E0124 20:08:31.777697       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:31.872073       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:31.872239       1 trace.go:236] Trace[90618979]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:21.807) (total time: 10064ms):
Trace[90618979]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10064ms (20:08:31.872)
Trace[90618979]: [10.064886162s] [10.064886162s] END
E0124 20:08:31.872278       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:31.912554       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:31.912728       1 trace.go:236] Trace[518808298]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:21.910) (total time: 10002ms):
Trace[518808298]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (20:08:31.912)
Trace[518808298]: [10.002119514s] [10.002119514s] END
E0124 20:08:31.912764       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:31.972175       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:31.972308       1 trace.go:236] Trace[1402465035]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:21.955) (total time: 10016ms):
Trace[1402465035]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10016ms (20:08:31.972)
Trace[1402465035]: [10.016279302s] [10.016279302s] END
E0124 20:08:31.972336       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:31.982902       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:31.983151       1 trace.go:236] Trace[1653395977]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:21.981) (total time: 10001ms):
Trace[1653395977]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (20:08:31.982)
Trace[1653395977]: [10.001648823s] [10.001648823s] END
E0124 20:08:31.983188       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:31.992532       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:31.992623       1 trace.go:236] Trace[876332391]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:21.990) (total time: 10002ms):
Trace[876332391]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (20:08:31.992)
Trace[876332391]: [10.002005712s] [10.002005712s] END
E0124 20:08:31.992642       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:32.023712       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:32.023884       1 trace.go:236] Trace[2124742265]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:22.021) (total time: 10002ms):
Trace[2124742265]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (20:08:32.023)
Trace[2124742265]: [10.002471276s] [10.002471276s] END
E0124 20:08:32.023925       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:32.063949       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:32.064055       1 trace.go:236] Trace[222401299]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:22.061) (total time: 10002ms):
Trace[222401299]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (20:08:32.063)
Trace[222401299]: [10.002264595s] [10.002264595s] END
E0124 20:08:32.064077       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:32.077456       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:32.077632       1 trace.go:236] Trace[565212965]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:22.074) (total time: 10002ms):
Trace[565212965]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (20:08:32.077)
Trace[565212965]: [10.002593845s] [10.002593845s] END
E0124 20:08:32.077663       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:32.082644       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:32.083168       1 trace.go:236] Trace[821445510]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:22.080) (total time: 10002ms):
Trace[821445510]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (20:08:32.082)
Trace[821445510]: [10.002687124s] [10.002687124s] END
E0124 20:08:32.083396       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:32.174071       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:32.174222       1 trace.go:236] Trace[189184978]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:22.171) (total time: 10002ms):
Trace[189184978]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (20:08:32.174)
Trace[189184978]: [10.002499219s] [10.002499219s] END
E0124 20:08:32.174250       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0124 20:08:32.174096       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:32.174338       1 trace.go:236] Trace[1388288308]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (24-Jan-2024 20:08:22.171) (total time: 10002ms):
Trace[1388288308]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (20:08:32.174)
Trace[1388288308]: [10.00273764s] [10.00273764s] END
E0124 20:08:32.174351       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0124 20:08:34.771806       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0125 19:00:41.109576       1 reflector.go:458] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: watch of *v1.ConfigMap ended with: an error on the server ("unable to decode an event from the watch stream: http2: client connection lost") has prevented the request from succeeding

* 
* ==> kube-scheduler [7c140e392dac] <==
* I0117 11:04:31.224090       1 serving.go:348] Generated self-signed cert in-memory
W0117 11:04:33.383080       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0117 11:04:33.383135       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0117 11:04:33.383148       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0117 11:04:33.383152       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0117 11:04:33.587805       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0117 11:04:33.587945       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0117 11:04:33.643555       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0117 11:04:33.643631       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0117 11:04:33.644109       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0117 11:04:33.644188       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0117 11:04:33.886913       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Jan 28 03:11:18 minikube kubelet[1805]: E0128 03:11:18.258349    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:11:22 minikube kubelet[1805]: E0128 03:11:22.128118    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:22 minikube kubelet[1805]: E0128 03:11:22.128248    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:27 minikube kubelet[1805]: E0128 03:11:27.136842    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:11:27 minikube kubelet[1805]: E0128 03:11:27.136925    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:11:32 minikube kubelet[1805]: E0128 03:11:32.184327    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:32 minikube kubelet[1805]: E0128 03:11:32.184455    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:34 minikube kubelet[1805]: E0128 03:11:34.127173    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:11:34 minikube kubelet[1805]: E0128 03:11:34.127262    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:11:42 minikube kubelet[1805]: E0128 03:11:42.137418    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:11:42 minikube kubelet[1805]: E0128 03:11:42.137556    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:11:42 minikube kubelet[1805]: E0128 03:11:42.239644    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:42 minikube kubelet[1805]: E0128 03:11:42.239781    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:47 minikube kubelet[1805]: E0128 03:11:47.328802    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:11:47 minikube kubelet[1805]: E0128 03:11:47.328870    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:11:52 minikube kubelet[1805]: E0128 03:11:52.300850    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:52 minikube kubelet[1805]: E0128 03:11:52.301013    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:11:54 minikube kubelet[1805]: E0128 03:11:54.137070    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:11:54 minikube kubelet[1805]: E0128 03:11:54.137182    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:12:00 minikube kubelet[1805]: E0128 03:12:00.272106    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:12:00 minikube kubelet[1805]: E0128 03:12:00.272186    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:12:02 minikube kubelet[1805]: E0128 03:12:02.359981    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:02 minikube kubelet[1805]: E0128 03:12:02.360130    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:06 minikube kubelet[1805]: E0128 03:12:06.135854    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:12:06 minikube kubelet[1805]: E0128 03:12:06.135946    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:12:12 minikube kubelet[1805]: E0128 03:12:12.418730    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:12 minikube kubelet[1805]: E0128 03:12:12.418851    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:16 minikube kubelet[1805]: E0128 03:12:16.226365    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:12:16 minikube kubelet[1805]: E0128 03:12:16.226441    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:12:21 minikube kubelet[1805]: E0128 03:12:21.136663    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:12:21 minikube kubelet[1805]: E0128 03:12:21.136737    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:12:22 minikube kubelet[1805]: E0128 03:12:22.477837    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:22 minikube kubelet[1805]: E0128 03:12:22.477992    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:30 minikube kubelet[1805]: E0128 03:12:30.128131    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:12:30 minikube kubelet[1805]: E0128 03:12:30.128217    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:12:32 minikube kubelet[1805]: E0128 03:12:32.535780    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:32 minikube kubelet[1805]: E0128 03:12:32.535926    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:35 minikube kubelet[1805]: E0128 03:12:35.135461    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:12:35 minikube kubelet[1805]: E0128 03:12:35.135547    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:12:42 minikube kubelet[1805]: E0128 03:12:42.599647    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:42 minikube kubelet[1805]: E0128 03:12:42.599849    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:43 minikube kubelet[1805]: E0128 03:12:43.351547    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:12:43 minikube kubelet[1805]: E0128 03:12:43.351645    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:12:48 minikube kubelet[1805]: E0128 03:12:48.140211    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:12:48 minikube kubelet[1805]: E0128 03:12:48.140293    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:12:52 minikube kubelet[1805]: E0128 03:12:52.669701    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:52 minikube kubelet[1805]: E0128 03:12:52.680956    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:12:55 minikube kubelet[1805]: E0128 03:12:55.146675    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:12:55 minikube kubelet[1805]: E0128 03:12:55.146781    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:13:00 minikube kubelet[1805]: E0128 03:13:00.141820    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:13:00 minikube kubelet[1805]: E0128 03:13:00.142042    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:13:02 minikube kubelet[1805]: E0128 03:13:02.766610    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:13:02 minikube kubelet[1805]: E0128 03:13:02.766735    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:13:08 minikube kubelet[1805]: W0128 03:13:08.382684    1805 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 28 03:13:08 minikube kubelet[1805]: E0128 03:13:08.424919    1805 kuberuntime_manager.go:1256] container &Container{Name:site,Image:briyankessel/site_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:4000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxn59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:frontend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod site-55857bc65-tcsfg_default(cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1): CreateContainerConfigError: configmap "frontend-config" not found
Jan 28 03:13:08 minikube kubelet[1805]: E0128 03:13:08.425033    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"site\" with CreateContainerConfigError: \"configmap \\\"frontend-config\\\" not found\"" pod="default/site-55857bc65-tcsfg" podUID="cc4deb6c-10db-4bb1-8426-1cbcedb1b0f1"
Jan 28 03:13:12 minikube kubelet[1805]: E0128 03:13:12.133856    1805 kuberuntime_manager.go:1256] container &Container{Name:api,Image:briyankessel/api_k8s_project,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3003,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{1 0} {<nil>} 1 DecimalSI},memory: {{536870912 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvncs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:backend-config,},Optional:nil,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod api-5ffb9d968f-65972_default(ab1a3b12-a7ee-4de4-b91c-324e67fc28a0): CreateContainerConfigError: configmap "backend-config" not found
Jan 28 03:13:12 minikube kubelet[1805]: E0128 03:13:12.133945    1805 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"api\" with CreateContainerConfigError: \"configmap \\\"backend-config\\\" not found\"" pod="default/api-5ffb9d968f-65972" podUID="ab1a3b12-a7ee-4de4-b91c-324e67fc28a0"
Jan 28 03:13:12 minikube kubelet[1805]: E0128 03:13:12.823877    1805 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"
Jan 28 03:13:12 minikube kubelet[1805]: E0128 03:13:12.824009    1805 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log\": failed to reopen container log \"d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/prod_mysql-tp-0_3a9307a7-b3f1-4016-b660-b8611d63b360/mysql/3.log" containerID="d5845e597f2fa523dc6eea50443e5e445c163aa595acecd139daa9733266c4ef"

* 
* ==> kubernetes-dashboard [60778a244ee1] <==
* 2024/01/24 20:09:26 Starting overwatch
2024/01/24 20:09:26 Using namespace: kubernetes-dashboard
2024/01/24 20:09:26 Using in-cluster config to connect to apiserver
2024/01/24 20:09:26 Using secret token for csrf signing
2024/01/24 20:09:26 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/01/24 20:09:27 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/01/24 20:09:27 Successful initial request to the apiserver, version: v1.28.3
2024/01/24 20:09:27 Generating JWE encryption key
2024/01/24 20:09:27 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/01/24 20:09:27 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/01/24 20:09:28 Initializing JWE encryption key from synchronized object
2024/01/24 20:09:28 Creating in-cluster Sidecar client
2024/01/24 20:09:28 Successful request to sidecar
2024/01/24 20:09:28 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [9499d036aa33] <==
* 2024/01/24 20:09:01 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": net/http: TLS handshake timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00069fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00015a200)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2024/01/24 20:09:01 Using namespace: kubernetes-dashboard
2024/01/24 20:09:01 Using in-cluster config to connect to apiserver
2024/01/24 20:09:01 Using secret token for csrf signing
2024/01/24 20:09:01 Initializing csrf token from kubernetes-dashboard-csrf secret

* 
* ==> storage-provisioner [0fbf552fcd61] <==
* I0125 19:00:58.353697       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0125 19:00:58.437094       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0125 19:00:58.437364       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0125 19:01:18.989229       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0125 19:01:18.989661       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_d6fd388b-987f-4e5e-9a52-682153cdbdc6!
I0125 19:01:18.995878       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ce3527c2-8fef-429e-84e0-d90fc9e679ec", APIVersion:"v1", ResourceVersion:"138584", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_d6fd388b-987f-4e5e-9a52-682153cdbdc6 became leader
I0125 19:01:19.290804       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_d6fd388b-987f-4e5e-9a52-682153cdbdc6!

* 
* ==> storage-provisioner [62703002a9eb] <==
* sync.(*Cond).Wait(0xc0004bcb40)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0001aa300, 0x0, 0x0, 0xc000545600)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc00044ac80, 0x18e5530, 0xc0003b4200, 0x203001)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0005a42a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0005a42a0, 0x18b3d60, 0xc00019c0f0, 0xc0001cad01, 0xc0001ca480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0005a42a0, 0x3b9aca00, 0x0, 0x1, 0xc0001ca480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0005a42a0, 0x3b9aca00, 0xc0001ca480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 111 [sync.Cond.Wait, 170 minutes]:
sync.runtime_notifyListWait(0xc0004bcb90, 0x11)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc0004bcb80)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0001aa480, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc00044ac80, 0x18e5530, 0xc0003b4200, 0x203001)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc0005a42c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0005a42c0, 0x18b3d60, 0xc000618fc0, 0x18b4601, 0xc0001ca480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc0005a42c0, 0x3b9aca00, 0x0, 0xc0004f4a01, 0xc0001ca480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc0005a42c0, 0x3b9aca00, 0xc0001ca480)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 18373 [chan receive]:
net/http.(*persistConn).addTLS(0xc000330c60, 0xc000044480, 0x9, 0x0, 0xc00004448a, 0x3)
	/usr/local/go/src/net/http/transport.go:1536 +0x21f
net/http.(*Transport).dialConn(0xc000445400, 0x18e5568, 0xc000044040, 0x0, 0xc0000c6080, 0x5, 0xc000044480, 0xd, 0x0, 0xc000330c60, ...)
	/usr/local/go/src/net/http/transport.go:1610 +0x1e05
net/http.(*Transport).dialConnFor(0xc000445400, 0xc0002b6000)
	/usr/local/go/src/net/http/transport.go:1442 +0xc6
created by net/http.(*Transport).queueForDial
	/usr/local/go/src/net/http/transport.go:1411 +0x40f

goroutine 18415 [chan send]:
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc0005e53c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:119 +0x32d
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

